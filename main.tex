\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[amsbook]{}
\usepackage[T2A]{fontenc}
\usepackage[russian]{}
\usepackage[hebibliography]{}
\usepackage{amssymb}
\usepackage{draftwatermark}
\SetWatermarkText{Draft}
\SetWatermarkScale{4}
\SetWatermarkColor[gray]{0.8}
\usepackage{physics}{}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{ยบ}

\title{Building a Space}
\author{Iurie Nistor}
\date{September 2021}

\begin{document}

\maketitle

\newpage
\emph{This work is under CC0 1.0 Universal https://creativecommons.org/publicdomain/zero/1.0/)}

\emph{To the extent possible under law, Iurie Nistor has waived all copyright and related or neighboring rights to Building a Space.}

\tableofcontents

\section{Foreword}

\section{Notations}

$L_n$ --  linear space of dimension $n$
\newline
$E_n$ --  affine space of dimension $n$
\newline
$R_n$ --  euclidean space of dimension $n$
\newline
$\{a, b, c, d...\}$ -- a set with elements $a, b, c, d, ...$

\chapter{Linear spaces}

In order to "build" a space we will start with the most abstract thing we can imagine - a \emph{set}, and let's call it $L$. The elements of the set $L$ can be of any nature: numbers, vectors, functions, anything, and named with letters $\{\textbf{a}, \textbf{b}, \textbf{c}, \textbf{d}...\}$. It is better we for now to try refrain from imagining them be something concrete, for example, as vectors we know from classic geometry books. Let pretend we don't know what they are, what to do with them, and how they behave.

\begin{section}{Axioms of linear space}

We will start now to define how the elements of the set $L$ behave, these rules are called \emph{the axioms of linear space}. Also these axioms are grouped as follows:


I: Axioms of sum

II: Axioms of multiplication to a number

Thus, we will define how to \emph{sum} the elements and how to \emph{multiply} them to real numbers.

Another concept related to the elements of $L$ is \emph{identity}. Thus, the elements of $L$ are \emph{identical} if they belong to the same class, i.e they have the same properties. Thus, we say that two elements $\textbf{a}$ and $\textbf{b}$ are identical (or equal) if they belong to $L$, have the same properties, and we note this $\textbf{a} = \textbf{b}$.

\begin{subsection}{I: Axioms of sum}

\begin{axiom} For every element $\textbf{a}$ and $\textbf{b}$ from $L$ the sum $\textbf{c} = \textbf{a} + \textbf{b}$ is defined in a such way that $\textbf{c}$ also belongs to $L$.
\end{axiom}

\begin{axiom}The sum of elements is commutative, i.e. for every $\textbf{a}$ and $\textbf{b}$ from $L$
$\textbf{a} + \textbf{b} = \textbf{b} + \textbf{a}$.
\end{axiom}

\begin{axiom} The sum of elements are associative, i.e. for every $\textbf{a}$, $\textbf{b}$ and $\textbf{c}$ from $L$
$(\textbf{a} + \textbf{b}) + \textbf{c} = \textbf{a} + (\textbf{b} + \textbf{c})$.
\end{axiom}

\begin{axiom} There exists an element {$\textbf{0}$} in $L$ such that for every $a$ from $L$  $\textbf{a} + \textbf{0} = \textbf{a}$.
\end{axiom}

\begin{axiom} For every element $\textbf{a}$ from $L$ there exists an element {$-\textbf{a}$} in $L$ such that $\textbf{a} + (-\textbf{a}) = \textbf{0}$.
\end{axiom}

\end{subsection}
\begin{subsection}{II: Axioms of multiplication to a number}
\begin{axiom} For every element $\textbf{a}$ from $L$ and for every real number $\alpha$ an element $\textbf{b}$ from $L$ is given as $\textbf{b} = \alpha \textbf{a}$, which is called the multiplication of $\textbf{a}$ to the number $\alpha$.
\end{axiom}

\begin{axiom} The multiplication of every element $\textbf{a}$ from $L$ to the number 1 is the same element \textbf{a}, i.e. $\textbf{a} \cdot 1 = \textbf{a}$.
\end{axiom}

\begin{axiom} The multiplication of elements of $L$ to a number is distributive relative to the sum of numbers, i.e. $(\alpha + \beta)\textbf{a} = \alpha \textbf{a} + \beta \textbf{a}$.
\end{axiom}

\begin{axiom}
The multiplication of elements of $L$ to a number is distributive relative to the sum of elements, i.e. $\alpha (\textbf{a} + \textbf{b}) = {\alpha} \textbf{a} + {\alpha} \textbf{b}$.
\end{axiom}

\begin{axiom} The multiplication of elements of $L$ to a number is associative, i.e. $\alpha (\beta \textbf{a}) = (\alpha \beta) \textbf{a}$.
\end{axiom}
\end{subsection}

Introduction of these axioms, i.e. rules upon the elements adds the first glimpses of our space we want to build, i.e. these first glimpses will transform the abstract set $L$ into a \emph{linear space}. Thus, here is the definition:

\emph{A set $L$ the elements of which follows the axioms of group I and II is called linear space $L$.}

A linear space, also, is also called \emph{vector space}, and the elements of $L$ are called \emph{vectors}. Due to this, starting from now, we will call the elements of the linear space $L$ vectors. Let's mention that even if the we them vectors they not necessarily represent vectors as we know from classic geometry.

If the axioms of multiplication define the multiplication of vectors with real numbers, than $L$ is called \emph{real linear space}. If the multiplication of vectors are with complex numbers than $L$ will be \emph{complex linear space}.
\end{section}

\section{Examples of linear spaces}

\subsection{Vectors on a two dimensional infinite plane}

Imagine all vectors (those defined like in the classic geometry books) which can be drawn on a infinite plane. Let's see if the set of these vectors $V$ forms a linear space. In order to verify this, there is a need two show that all vectors of the plane $P$ follow the axioms of the linear space.

Let's verify the fist axiom from the group I and II. If we draw on the plane $P$ two vectors $\textbf{a}$ and $\textbf{b}$ and sum them by using the rule of parallelogram we find another vector $\textbf{c} = \textbf{a} + \textbf{b}$ and which lies in the same plane, i.e. $c \in V$. If we draw a vector $\textbf{d}$ and multiply them to a number $\alpha$ we will find another vector $\textbf{e}$ which is collinear to $\textbf{d}$ and lies on the same plane, i.e. $\textbf{d} \in V$.

The rest of the axioms can be verified the same way by using the rules for vectors from the classic geometry. Thus, all vectors from an infinite plane form a linear space.

Because, for example, values from Physics like velocities, accelerations, forces and others are denoted and behave same way as vectors they form linear spaces too.

\subsection{The space of real numbers}

The set of all real numbers $R$ form a linear space. Indeed, for any two numbers from this set we can find the sum which is in the same set. Also, there is \textbf{0} vector which is 0, and for every number we can find its opposite number by adding the sing '-' before the number. Multiplication to a number also hods. Thus, we can see that the elements of the set $R$ forms a linear space.

\subsection{The space of matrices}

\subsection{Null space}

\subsection{The space of functions of a single variable}

Let's be there a set $F$ the elements of which are all functions of a single variable $x$. The set $F$ forms a vector space.

To verify the axiom of sums (I) we take two elements $f_a(x) \in F$ and $f_b(x) \in F$ and define a new function $f_c(x) = f_a(x) + f_b(x)$. It can seen that $f_c(x)$ is also a function of a single variable $x$ and $f_c(x) \in F$. For the rest of the axioms of sum are obvious.

The multiplication axioms (II) are also easy to verify. For any $f(x) \in F$ and a real number $\alpha$ we can define a new function $f_\alpha (x) = \alpha \cdot f(x)$ which $f_\alpha (x) \in F$. For the rest of the axioms of multiplication are obvious too.

Note that the elements of $F(f_a(x), f_b(x), f_c(x)...)$ are called vectors too, they form a vector space but is not really intuitive to image them as vectors.

Functions which form vector spaces are used as a mathematical tool in Quantum Mechanics. These kind of spaces are called Hilbert spaces\footnote{Hilbert space is actually an infinite dimensional linear space\cite{hobson}. We will see later what is a dimension of a linear space.}, the vectors of which often are noted with $\ket{a}, \ket{b}, \ket{c}...$, called ket vectors\cite{dirac} and which represent the state of a quantum system. Thus, the state of a quantum system forms a linear space and, because the axioms I are hold, than it means the the state of a quantum system can be expressed as a sum of other states\footnote{In Quantum Mechanics it is called the principle of superposition of quantum states.\cite{dirac}}.

\subsection{Vectors on a finite plane}

All vectors of a finite plane $P$ do not form a vector space. Indeed, we can take two vectors $\textbf{a}$ and $\textbf{b}$ from the plane in a such way that sum $\textbf{c} = \textbf{a} + \textbf{b}$ will not fit $\textbf{c}$ into to plane, i.e $\textbf{c} \notin P$. Thus, at least the first axiom of the group I doesn't hold.

\subsection{Natural numbers}

\section {Some corollaries to the axioms of a linear space}

TEXT HERE

\section{Linear combination and dependence}

Because the axioms of linear space defines how we can sum elements and multiply to a real number now we can define some more complex concepts related to linear space. 

Let's have a finite number of elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ and any real numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$. If any element $\boldsymbol{b} \in L$ is defined as

\[\boldsymbol{b} = \alpha_1\boldsymbol{a_1} + \alpha_2\boldsymbol{a_2} + \alpha_3\boldsymbol{a_3} + ... + \alpha_k\boldsymbol{a_n},\]

we say that $\boldsymbol{b}$ is expressed as a \emph{linear combination} of $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_k}$. If all the numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$ are zero than the linear combination is called \emph{trivial linear combination}, otherwise (if at least one number is not zero) it is called \emph{non-trivial linear combination}.

Any elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ are \emph{linearly dependent} if there exists a non-trivial combination such that

\begin{equation}
\label{eqn:dependence}
\alpha_1\boldsymbol{a_1} + \alpha_2\boldsymbol{a_2} + \alpha_3\boldsymbol{a_3} + ... + \alpha_n\boldsymbol{a_n} = \boldsymbol{\theta}.
\end{equation}

On the other side the elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ are \emph{linear independent} if \eqref{eqn:dependence} holds only for the trivial combination, i.e. only when the real numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$ are all equal to zero.

From these two above definitions it follows that if any set $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ which are linear dependent there exists an element $\boldsymbol{a_k}$ from this set which can be expressed as a liner combination of the rest:

\[\boldsymbol{a_k} = \frac{\alpha_1}{\alpha_k}\boldsymbol{a_1} + \frac{\alpha_2}{\alpha_k}\boldsymbol{a_2} + \frac{\alpha_3}{\alpha_k}\boldsymbol{a_3} + ... +  \frac{\alpha_{k-1}}{\alpha_k}\boldsymbol{a_{k-1}} + \frac{\alpha_{k+1}}{\alpha_k}\boldsymbol{a_{k+1}} + ... + \frac{\alpha_n}{\alpha_k}\boldsymbol{a_n} ,\]

where $\alpha_k \neq 0$. Indeed, because the elements are linear depended than their combination is non-trivial, i.e. there must exist at least one element $a_k$ for which $\alpha_k$ is not null. 

For any set of elements from $L$ which are linear independent, than no element from this set can be expressed as a linear combination of others.

\begin{section}{The dimension and basis of the linear space}

The linear space $L$ is said to be $n-dimensional$ if there exist a finite $n$ elements from $L$ which are linear independent and every other number of elements which is bigger than $n$ is linear dependent.

The linear space would be an \emph{infinite dimensional linear space} if for any integer $N > 0$ we can find $N$ linear independent elements from $L$.

As we can see the dimension of a linear space is not the same as how many elements are in $L$.

If the elements $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ from $L$ are linear independent and every other element $\boldsymbol{a}$ from $L$ can be expressed as a linear combination: 
\begin{equation}
\label{eqn:basis}
\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + a_3\boldsymbol{e_3} + ... + a_n\boldsymbol{e_n},
\end{equation}
than $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ is called to from a \emph{basis} in $L$.

Every element which is expressed in the form of \ref{eqn:basis} is said that is expressed as a linear combination of the basis $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$. The coefficients $a_1, a_2, a_3,..., a_n$ are called the coordinates of the element $\boldsymbol{a}$ in this basis.

The basis $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ is not unique, there can be a lot of other set of elements which will form a basis.

We should not think $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ as unitary geometric vectors. They might be if we consider the elements to be geometric vectors and the basis with unitary vectors like $\boldsymbol{i}, \boldsymbol{j}, \boldsymbol{k}$. For example, in the three dimensional space of geometric vectors any three vectors (not necessarily orthogonal and unitary) which are not all coplanar can form a basis, i.e. every other vectors can be expressed in this basis.

Thus, in a $n$-dimensional linear space $L$ we can take any $n$ linear independent elements (if such exists) and form a basis. Also, these theorem are true:

\emph{The linear space has the dimension $n$ if and only if there exists a basis of n elements.}

\emph{If there exists a basis in $L$ than any element $\boldsymbol{a}$ which is not part of the basis is expressed uniquely in this basis, i.e. the coordinates of the element $\boldsymbol{a}$ in this basis are unique.}

Also here we have another important theorem:

\emph{The coordinates of sum of two vectors are the sum of coordinates of these vectors. If a vector is multiplied by a real number, also the coordinates of the vector are multiplied by this number.}

To be more clear if we have

\[\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + a_3\boldsymbol{e_3} + ... + a_n\boldsymbol{e_n},\]
\[\boldsymbol{b} = b_1\boldsymbol{e_1} + b_2\boldsymbol{e_2} + b_3\boldsymbol{e_3} + ... + b_n\boldsymbol{e_n}\]

than

\[\boldsymbol{a} + \boldsymbol{b} = (a_1 + b_1)\boldsymbol{e_1} + (a_2 + b_2)\boldsymbol{e_2} + ... + (a_n + b_n)\boldsymbol{e_n},\]
\[{\alpha}\boldsymbol{a} = ({\alpha}a_1)\boldsymbol{e_1} + ({\alpha}a_2)\boldsymbol{e_2} + ... + ({\alpha}a_n)\boldsymbol{e_n}.\]

Also, it follows that all the coordinates of the null element are zero, i.e:

\[\boldsymbol{\theta} = 0{\cdot}\boldsymbol{e_1} + 0{\cdot}\boldsymbol{e_2} + ... + 0{\cdot}\boldsymbol{e_n}.\]
\end{section}

\section{Isomorphism}

Let's have linear spaces $L$ and $L^'$. The elements of $L$ and $L^'$ might be of the same or different nature \footnote{For example, $L(\boldsymbol{v_1}, \boldsymbol{v_2}, \boldsymbol{v_3},...)$ might be a space of vectors and $L^{'}(f_1(x), f_2(x), f_3(x) ...)$ a space of functions of a single variable.}. Also, there a bidirectional unique mapping between the elements $L$ and ${L^{'}}$, i.e. for every element $\boldsymbol{a}$ from $L$ we assign an element $\boldsymbol{a^{'}}$ from $L$ and the opposite. The mapping is done in a such way that the sum of any two elements $\boldsymbol{a}$ and $\boldsymbol{b}$ from $L$ maps to the sum of $\boldsymbol{a^{'}}$ and $\boldsymbol{b^{'}}$ from $L^{'}$, and the multiplication of any element $\boldsymbol{a}$ from $L$ to a real number $\alpha$ maps to the multiplication of $\boldsymbol{a^{'}}$ from $L^{'}$ to the same number, i.e.

\begin{equation}
\label{eqn:isomorphism1}
(\boldsymbol{a} + \boldsymbol{b})^{'} = \boldsymbol{a^{'}} + \boldsymbol{b^{'}},
\end{equation}

\begin{equation}
\label{eqn:isomorphism2}
(\alpha \cdot \boldsymbol{a})^{'} = \alpha \cdot \boldsymbol{a^{'}}.
\end{equation}

The spaces $L$ and $L^{'}$ are said to be \emph{isomorphic} if they are mapped and the mapping is done in a such way that \eqref{eqn:isomorphism1} and \eqref{eqn:isomorphism2} holds\footnote{The linear spaces must be both real or complex. If one space is real and another complex, the spaces are not isomorphic because \eqref{eqn:isomorphism2} doesn't hold.}.

\begin{theorem}
For every n all real n-dimensional linear spaces are isomorphic between each other. For every n all complex n-dimensional linear spaces are isomorphic between each other.
\end{theorem}

\begin{theorem}
The linear space which is isomorphic to a n-dimensional linear space is n-dimensional too.
\end{theorem}

Here are also two corollaries which follows from these theorems:

\begin{corollary}
Finite dimensional linear space with different dimension are not isomorphic.
\end{corollary}

For example, the space of geometric vectors on an infinite plane is of dimension two. The linear space of vectors which are on an infinite line is of dimension one. These two linear spaces are not isomorphic.

\section{Linear subspaces}

Let's have a linear space $L$. Also, let's take a set of elements from $L$ and call it $\widetilde{L}$.

\begin{definition}
The set $\widetilde{L}$ of elements from $L$ is called linear subspace of $L$ if the axioms of addition and multiplication hold upon the set $\widetilde{L}$.
\end{definition}

This would mean that for every element $\boldsymbol{a}$ and $\boldsymbol{b}$ from $\widetilde{L}$ the sum $\boldsymbol{c} = \boldsymbol{a} + \boldsymbol{b}$ belongs to $\widetilde{L}$ too. The result of multiplication of an element $\boldsymbol{a}$ from $\widetilde{L}$ to a real number also belongs to $\widetilde{L}$, i.e if $\boldsymbol{b} = \alpha\cdot\boldsymbol{a}$ than $\boldsymbol{b} \in \widetilde{L}$.

Following the definition:

\begin{theorem}
Every linear subspace $\widetilde{L}$ of a linear space $L$ also is a linear space.
\end{theorem}

This is true because every $\widetilde{L}$ satisfies the axioms I and II.

To understand better how $\widetilde{L}$ looks like let's take the example of the geometric vectors on an infinite plane. At first glance the word "subspace" makes us to think that if we will take a finite region, for example, a circle on this plane than that would be a subspace. It is not true because all vectors that can be put on this region will not satisfy the axioms of linear space. What would be than a linear subspace? An infinite line that lies on the plane would be a linear space. Indeed, all vectors which can lie on this line will satisfy the axioms of linear space, and all these vectors also lie on the plane, i.e. they are a subset of linear space of vectors on the plane.

Another trivial example of linear subspace will be the null element $\boldsymbol{\theta}$ of a linear space.

\section{Linear transformation}

\subsection{Einstein notation}

Before starting the subject of linear transformation let's define some notations which are called Einstein summation convention\footnote{This kind of notation is often used in Physics. It was introduced by Albert Einstein in 1916.}. The reason of introduction of such notation is because we can write mathematical formulas in a more efficient way\footnote{For example, the Physics equations will look more simple and clear.}. Let's write some examples.

\begin{equation}
\label{eqn:enstein_notation1}
a_kb_k := \sum_{k=1}^{n} a_kb_k = a_1b_1 + a_2b_2 + ... + a_nb_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation2}
a_{ik}b_k := \sum_{k=1}^{n} a_{ik}b_k = a_{i1}b_1 + a_{i2}b_2 + .. + a_{in}b_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation3}
a_{ij}b_j := \sum_{k=1}^{n} a_{ij}b_j = a_{i1}b_1 + a_{i2}b_2 + .. + a_{in}b_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation4}
a_{ij}b_{jk}c_k := \sum_{j=1}^{m}\sum_{k=1}^{n} a_{ij}b_{jk}c_k.
\end{equation}

Einstein notation says that if the same index is
found in different terms, than the summation
is done over that index which is called the \emph{dummy}
index. Thus, instead of writing sums like $\sum_{k=1}^{n} a_kb_k$ we use a short notation $a_kb_k$ instead. Thus, in \eqref{eqn:enstein_notation1} $k$ is the dummy index over which the summation is done and because is found in both terms $a$ and $b$. In \eqref{eqn:enstein_notation2} the dummy index is $j$ over which the summation is done and 
because is found in both terms. In \eqref{eqn:enstein_notation3} we have two dummy indexes $j$ and $k$.

The name of the dummy index can be changed the result will be unchanged. For example

\[a_{ij}b_j = a_{iq}b_q,\]

because we in both cases we will have to sum starting from $1$ to $n$. The only think this kind of notation doesn't tell us is from which number to start and end the sum. Thus, using this kind of notation there should be an agreement upon this.

\subsection{The basis and coordinates transformation}

Let bet there a linear space $L_n$. The lower index means that it $n$-dimensional. The basis in this space will be $e_1, e_2, ..., e_n$. Because there may be a lot of other sets of elements from $L_n$ that can from a basis we are interested if we change the basis how the coordinates of an element from $L_n$ are changed.

First of all let express the new basis vectors in terms of old basis:

\begin{equation}
\label{eqn:basis_transfromation}
\begin{split} 
\boldsymbol{e^{'}_1} &= P_{11}\boldsymbol{e_1} + P_{12}\boldsymbol{e_2} + ... + P_{1n}\boldsymbol{e_n} \\
\boldsymbol{e^{'}_2} &= P_{21}\boldsymbol{e_1} + P_{22}\boldsymbol{e_2} + ... + P_{2n}\boldsymbol{e_n} \\
 &\;\;\vdots \\
\boldsymbol{e^{'}_n} &= P_{n1}\boldsymbol{e_1} + P_{n2}\boldsymbol{e_2} + ... + P_{nn}\boldsymbol{e_n}
\end{split}
\end{equation}

Using the Einstein notation we now can write this transformation as:

\begin{equation}
\label{eqn:basis_transfromation1}
\boldsymbol{e^{'}_i} = P_{ij}\boldsymbol{e_j}.
\end{equation}

The equation \eqref{eqn:basis_transfromation1} actually shows how basis vectors are changed if the basis is changed. The the coefficients $P_{ij}$ form a matrix which is called the transformation matrix of basis vectors. On the other hand knowing a basis and if we take any matrix $P$ in a such way that 

\[Det P \neq 0\],

than using \eqref{eqn:basis_transfromation1} we can find a new basis in $L_n$.

Transformation \eqref{eqn:basis_transfromation1} can be written in matrix notation of we choose to note the set of basis vectors a on column matrix:

\begin{equation}
\label{eqn:basis_transfromation2}
\boldsymbol{E^{'}} = P\boldsymbol{E}.
\end{equation}

Now let's see how the coordinates of a vector is are changed if we changed the basis. Let there be vector $\boldsymbol{a} \in L_n$. Because the vectors $\boldsymbol{a}$ itself doesn't change if the basis is changed than we can express this vectors in old and new basis as follows:

\begin{align*}
\boldsymbol{a} &= a_i\boldsymbol{e_i} \\ \boldsymbol{a} &= a^{'}_i\boldsymbol{e{'}_i}.
\end{align*}

In the second expression using \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e{'}_i}$ we can write:

\[\boldsymbol{a} &= a^{'}_i(P_{ij}\boldsymbol{e_j}) = a^{'}_i(P_{ji}\boldsymbol{e_i}) = (a^{'}_{i}P_{ji})\boldsymbol{e_i} \]\footnote{Here we sum over dummy index $i$ and $j$ because they are met in different terms, i.e. $\sum_{i=1}^{n} \sum_{j=1}^{n}a^{'}_iP_{ij}\boldsymbol{e_j}$ }.

Comparing this with the first expression we find the expression if old coordinates of a vectors in the of new coordinates:

\begin{equation}
\label{eqn:transfromation_coordinates}
a_i = P_{ji}a^{'}_j.
\end{equation}

From \eqref{eqn:transfromation_coordinates} we can see the the transformation matrix for coordinates is the transposed $P^T$ matrix $P$ of \eqref{eqn:basis_transfromation1}. If we not the collection of coordinates as one column matrix, we can write \eqref{eqn:transfromation_coordinates} by using matrix notation:

\begin{equation}
\label{eqn:transfromation_matrix1}
A = P^{T}A^{'}.
\end{equation}

If we want to express the new coordinates in terms of old we can write:

\begin{equation}
\label{eqn:transfromation_matrix2}
A^{'} = (P^{T})^{-1}A.
\end{equation}

If $(P^T)^{-1}$ is noted as $Q$ than

\begin{equation}
\label{eqn:transfromation_matrix3}
A^{'} = QA.
\end{equation}

Or using coordinates

\begin{equation}
\label{eqn:transfromation_coordinates1}
a{'}_i = Q_{ij}a_j.
\end{equation}

Comparing \eqref{eqn:basis_transfromation2} we can see the the coordinates of a vector from $L_n$ transforms using a different transformation matrix $(P^T)^{-1}$ when we move from an old basis $\boldsymbol{E}$ to a new one $\boldsymbol{E^'}$. This matrix can be obtained from the $P$ by taking the inverse of the transpose. Also, $det ((P^T)^{-1}) \neq 0$.

If we are given the matrix $(P^T)^{-1}$ we can find $P$ and then find the new basis by using \eqref{eqn:basis_transfromation2}. i.e. if we have the transformation law for vector coordinates we can find the transformation law for the basis.

\chapter{Affine space}

\section{The definition of affine space}
Now another step is taken and add more to the construction of the space,
i.e. to its geometry. For example, if only a subset of vectors from the linear space
is taken than with it we cannot define objects. There is a need for something more.
Beside the linear space $L$ there is given another set $\mathbb{A}$ the elements of which are called \emph{points} and can be of any nature. The points of this set be noted as A, B, C.. etc.

The \emph{affine space} is the set $\mathbb{A}$ of points for which the mapping of points to vectors from the linear space $L$ are stated by the following axioms (grouped as III):

\begin{axiom}
For every two points A and B from $\mathbb{A}$ we map a vector \boldsymbol{a} from L as follows:

\[\overline{AB} = \boldsymbol{a}.\]

If order of the points are taken as \overline{BA}, than

\[\overline{BA} = -\boldsymbol{a}.\]
\end{axiom}

\begin{axiom}
For every point $A \in \mathbb{A}$ and every vector $\boldsymbol{a} \in L$ an unique point $B \in \mathbb{A}$ is found such that

\[\overline{AB}\ = \boldsymbol{a}.\]

\end{axiom}

\begin{axiom}
If for any points A, B, C from $\mathbb{A}$ is mapped $\overline{AB} = \boldsymbol{a}$, $\overline{BC} = \boldsymbol{b}$ than $\overline{AC}$ will map to $\boldsymbol{a} + \boldsymbol{b}$, i.e.  $\overline{AC} = \boldsymbol{a} + \boldsymbol{b}$.
\end{axiom}

The dimension of the affine space is the dimension of its linear space $Ln$. The affine space is real or complex if the linear space $L$ is real or complex.

\section{Affine coordinates}

Let's have an affine space $\mathbb{A}$ of dimension $n$. Let's take point $O \in \mathbb{A}$ which will be called \emph{origin of affine coordinates}. Also, in the corresponding linear space $L$ a basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$ is taken. If another point $P \in \mathbb{A}$ is taken than $\overline{OP}$ will correspond to vector from $L_n$.
If we expand this vector $\overline{OP}$ in the basis than:

\begin{equation}
\overline{OP} = x_1\boldsymbol{e_1} + x_2\boldsymbol{e_2} + ... + x_n\boldsymbol{e_n} = x_i\boldsymbol{e_i}.
\end{equation}

The coefficients $x_1, x_2, ..., x_n$ are called the \emph{affine coordinates} of the point $P$ relative to the affine origin $O$. Thus, the affine system of coordinates always is defined by the chosen point $O$ and the basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$. Let's note this system as $(O, \boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n})$.

\section{The relationship between affine coordinates and vector coordinates}

Let's have the coordinate system in affine space $(O, \boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n})$ and two points $X$ and $Y$. The corresponding affine coordinates of the points $X$ and $Y$ relative to the origin $O$ will be $\{x_1, x_2, ..., x_n\}$ and $\{y_1, y_2, ..., y_n\}$. Also, the corresponding vector of these two points will be $\boldsymbol{a} = \overline{XY} \in L$. An important point here is that we want to see in affine space what is the relationship between the affine coordinates of the points $X, Y$ and the corresponding coordinates of the vector $\boldsymbol{a} \in L$. In this way we will be able to span the vector $\boldsymbol{a}$ in terms of the basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$ and the affine coordinates of the points $X, Y$ which marks the beginning and the end of the vector $\boldsymbol{a}$. This relationship would "glue" even more more the vector space $L$ and the set of points $\mathbb{A}$ into a singe space - affine space.

First lets span the vector $\boldsymbol{a}$ in the basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$:

\begin{align}
\boldsymbol{a} &= \overline{XY} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + ... + a_n\boldsymbol{e_n}. \label{affine_coordinates_vector_span}
\end{align}

On the other hand because of the point $O$ we can write the vector $\overline{XY}$ as

\begin{align*}
\overline{XY} &= \overline{XO} + \overline{OY} = \overline{OY} - \overline{OX}, \\
\overline{XY} &= \overline{OY} - \overline{OX} = y_1\boldsymbol{e_1} + y_2\boldsymbol{e_2} + ... + y_n\boldsymbol{e_n} - (x_1\boldsymbol{e_1} + x_2\boldsymbol{e_2} + ... + x_n\boldsymbol{e_n}),
\end{align*}

and finally

\begin{align}
\overline{XY} &= (y_1 - x_1)\boldsymbol{e_1} + (y_2 - x_2)\boldsymbol{e_2} + ... + (y_n - x_n)\boldsymbol{e_n}. \label{vector_coordinates_span}
\end{align}

Comparing \eqref{vector_coordinates_span}  with \eqref{affine_coordinates_vector_span} we can write the coordinates of the vector $\boldsymbol{a}$ as

\begin{align}
a_i = y_i - x_i. \label{vector_span_as_affine_coordinates}
\end{align}

According with the definition of the affine coordinates the coordinates $x_i$ and $y_i$ of the vectors $\overline{OY}$ and $\overline{OX}$ are also the coordinates of the points X, and Y. Taking also in consideration \eqref{vector_span_as_affine_coordinates} we can say that \emph{the difference of the affine coordinates of any two points from $\mathbb{A}$ will correspond to the coordinates of a vector from $L$ to which these two points are mapped to}. Thus, the vector $\boldsymbol{a}$ can be spanned as:

\begin{align}
\boldsymbol{a} = (y_1 - x_1)\boldsymbol{e_1} + (y_2 - x_2)\boldsymbol{e_1} + ... + (y_n - x_n)\boldsymbol{e_n} = (y_i - x_i)\boldsymbol{e_i}. \label{vector_span_affine_coordiantes}
\end{align}

It is important to note that the relation \eqref{vector_span_affine_coordiantes} make sense only if we have chosen the origin $O$ in the affine space.

\section{Transformation of affine coordinates}

Let's have two points A and B with coordinates ${a_1, a_2, ..., a_n}$ and ${b_1, b_2, ..., b_n}$ than we are interested what will be the coordinates of the vector $\overline{AB}$ expressed as coordinates of the points $A$ and $B$. We can write the following:

\[\overline{AB} = \overline{OB} - \overline{OA} = b_i\boldsymbol{e_i} - a_i\boldsymbol{e_i} = (b_i - a_i)\boldsymbol{e_i}. \] 

Let not see how the coordinates of a point $A$ changes if we move the origin $O$ to another point $O_1$. The coordinates of the point $O_1$ relative old origin will be ${o_1, o_2, ..., o_n}$. The coordinates of the point $A$ relative the origin $O$ will be ${a_1, a_2, ..., a_n}$. Let's find the coordinates ${a{^'}_1, a{^'}_2, ..., a{^'}_n}$ of the point $A$ relative to the new origin $O_1$. First we write the vector relation:

\[\overline{OA} = \overline{OO_1} + \overline{O_1A}\]

Now let's write this in term of coordinates:

\begin{align}
a_i\boldsymbol{e_i} = o_i\boldsymbol{e_i} + a^{'}_i\boldsymbol{e_i} = (o_i + a^{'}_i)\boldsymbol{e_i}.\label{affine_transfrom}
\end{align}

Than we obtain:

\begin{align}
a_i &= o_i + a^{'}_i \label{affine_origin_move} \\
a^{'}_i &= a_i - o_i. \label{affine_origin_move1}
\end{align}

Suppose that we move from one coordinates system $(O, \boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n})$ to another $(O_1, \boldsymbol{e^{'}_1}, \boldsymbol{e^{'}_1}, ..., \boldsymbol{e^{'}_n})$, i.e. we change not only the origin but also the basis.

Again lets write the vector relation which doesn't depend of the affine coordnates system:

\[\overline{OA} = \overline{OO_1} + \overline{O_1A}\]

Now we expand the vectors $\overline{OA}$ and $\overline{OO_1}$ in the old system but $\overline{O_1A}$ in the new system. The relation will hold too because the vectors are not changed. Thus we obtain something similar to \eqref{affine_transfrom}:

\begin{align}
a_i\boldsymbol{e_i} = o_i\boldsymbol{e_i} + a^{'}_i\boldsymbol{e^{'}_i} \label{affine_transfrom1}.
\end{align}


Using the \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e^{'}_i}$ in \eqref{affine_transfrom1} we can write:

\begin{align}
a_i\boldsymbol{e_i} &= a^{'}_{i}P{ij}\boldsymbol{e_j} + o_i\boldsymbol{e_i} = P{ji}a^{'}_{i}\boldsymbol{e_i} + o_i\boldsymbol{e_i} \label{affine_transfrom2}.\\
a_i &= Pjia^{'}_i + o_j. \label{affine_transfromation1}
\end{align}

Now let's write \eqref{affine_transfromation1} in the matrix from:

\begin{align}
A &= P^{T}A^{'} + O \\
\end{align}

From the matrix equation we can find matrix $A^'$:

\begin{align*}
A^{'} = (P^T)^{-1}A - (P^T)^{-1}O = QA - QO.
\end{align*}

In index notation this will be:

\begin{align}
a^{'}_i = Q_{ij}a_j - Q_{ij}o_j \label{general_affine_tr}
\end{align}

If the basis is not changed and only the origin $O$, it means that the matrix Q is unitary, thus,

\[a^{'}_i = I_{ij}a_j - I_{ij}o_j = a_i - o_i.\]

Thus, we arrived to \eqref{affine_origin_move1}.

If the basis is changed and the origin is the same, than in \eqref{general_affine_tr} all coefficients $o_j$ are zero than

\begin{align}
a^{'}_i = Q_{ij}a_j \label{affine_transform_without_origin}
\end{align}

thus, we arrived to \eqref{eqn:transfromation_coordinates1}.

\section{Objects in affine space}

From our experience we know that in our real space we have objects, in classic geometry they are called geometric object. It is natural for as to ask since we are talking about a space what are the objects in this space? How do they look like? Because in the affine space we added a set of points beside the linear vector space $L$, it is possible know to define what would be a object in affine space. In general, an object D in the affine space would consist of a set of points $D$ from $\mathbb{A}$.

\section{The volume of object in affine space}

\chapter{Linear and bilinear forms}

\section{Linear form}

Let's have a linear space $L$. For every vectors $\boldsymbol{a} \in L$ we associate a real number. In this way we are defining a scalar function of vector argument $f(\boldsymbol{a})$ over the space $L$. This function will be called $linear$ if it satisfies the following relations:

\begin{align}
f(\boldsymbol{a} + \boldsymbol{b}) &= f(\boldsymbol{a}) + f(\boldsymbol{b}) \textnormal{ for every } \boldsymbol{a} \in L \textnormal{ and } \boldsymbol{b} \in L\textnormal{,} \label{linearfunction_1} \\
f(\alpha\boldsymbol{a}) &= \alpha f(\boldsymbol{a}) \textnormal{ for every vector } \boldsymbol{a} \in L \textnormal{ and any real number } \alpha. \label{linearfunction_2}
\end{align}

If the space $L$ is complex than the function value $f(\boldsymbol{a})$ will be a complex number.

Now let's have in $L$ a basis $\mathscr{E}$ and a vector $\boldsymbol{a} \in L$ expressed in this basis:

\begin{align*}
\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + ... + a_n\boldsymbol{e_n}.
\end{align*}

The linear function $f(\boldsymbol{a})$ taking in consideration \eqref{linearfunction_1} and \eqref{linearfunction_2} can be expressed as:

\begin{align*}
f(\boldsymbol{a}) = a_{1}f(\boldsymbol{e_2}) + a_{2}f(\boldsymbol{e_2}) + ... + a_{n}f(\boldsymbol{e_n}) = a_if(\boldsymbol{e_i}). \label{linear_from0}
\end{align*}

If we write the coefficients $f(\boldsymbol{e_i})$ as $l_i$ than

\begin{align}
f(\boldsymbol{a}) = l_{1}a_{1} + l_{2}a_{2} + ... + l_{n}a_{n} = l_{i}a_{i}. \label{linear_from1}
\end{align}

The expression \eqref{linear_from1} is a polynomial of degree 1 relative to $a_i$ and it is called \emph{linear from}.

Now let's see how the coefficients $l_i$ transforms if we are moving to another base $E^'$. In the base $E^'$ we have:

\begin{align}
f(\boldsymbol{a}) &= f(a^{'}\boldsymbol{e^{'}_i}) = a^{'}_if(\boldsymbol{e^{'}_i}) = a^{'}_{i}l^{'}_i \label{linear_from_new_basis}
\end{align}

In accordance with \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e^{'}_i}$ we write:

\begin{align*}
f(\boldsymbol{a}) &= a^{'}_if(\boldsymbol{e^{'}_i}) = a^{'}_if(P_{ij}\boldsymbol{e_j}) =  a^{'}_iP_{ij}f(\boldsymbol{e_j}) = a^{'}_iP_{ij}l_j.\label{linear_from0}
\end{align*}

Comparing with \eqref{linear_from_new_basis} we can see that the expression of transformation for coefficients $l_i$ is:

\begin{align}
l_i^{'}&= P_{ij}l_j,\label{covariant_transformation}
\end{align}

i.e. they transform like basis vectors.

It is worth mentioning that the value of function $f(\boldsymbol{a})$ doesn't change if the basis is changed.

\section{Bilinear form}

If for every two vectors $\boldsymbol{a} \in L$ and $\boldsymbol{b} \in L$ we associate a real number, than we are defining a function of two vector arguments $f(\boldsymbol{b}, \boldsymbol{a})$ over the space $L$. The function $f(\boldsymbol{b}, \boldsymbol{a})$ will be called \emph{bilinear} if is is linear in respect to every its argument, i.e:

\begin{align*}
f(\boldsymbol{a_1} + \boldsymbol{a_2}, \boldsymbol{b}) &= f(\boldsymbol{a_1}, \boldsymbol{b}) + f(\boldsymbol{a_2}, \boldsymbol{b}), \\
f(\alpha\boldsymbol{a_1}, \boldsymbol{b}) &= \alpha f(\boldsymbol{a}, \boldsymbol{b}), \\
f(\boldsymbol{a}, \boldsymbol{b_1} + \boldsymbol{b_2}) &= f(\boldsymbol{a}, \boldsymbol{b_1}) + f(\boldsymbol{a}, \boldsymbol{b_2}), \\
f(\boldsymbol{a}, \alpha\boldsymbol{b}) &= \alpha f(\boldsymbol{a}, \boldsymbol{b}).
\end{align*}

If we use the above relation than we can expand $f(\boldsymbol{a}, \boldsymbol{b})$ as follows:

\begin{align*}
f(\boldsymbol{a}, \boldsymbol{b}) = f (a_i\boldsymbol{e_i}, b_j\boldsymbol{e_j}) = a_{i}b_{j}f(\boldsymbol{e_i}, \boldsymbol{e_j}) = l_{ij}a_{i}b_{j} .
\end{align*}

The expression $l_{ij}a_{i}b_{j}$ is called \emph{bilinear form}. The coefficients $l_{ij}$ are called \emph{coefieints of the bilinear form} and they can be written as a matrix, called the \emph{matrix of the bilinear form}.

Now let's see how the coefficients $l_{ij}$ transform with change of basis. In the new basis

\begin{align}
f(\boldsymbol{a}, \boldsymbol{b})
= f (a^{'}_i\boldsymbol{e^{'}_i}, b^{'}_j\boldsymbol{e^{'}_j})
= a^{'}_i b^{'}_j f(\boldsymbol{e^{'}_i}, \boldsymbol{e^{'}_j})
= l^{'}_{ij} a^{'}_i b^{'}_j  . \label{bilinear_transform}
\end{align}

Using \eqref{eqn:basis_transfromation1} we can write:

\begin{align*}
f(\boldsymbol{a}, \boldsymbol{b}) &= a^{'}_i a^{'}_j f(\boldsymbol{e{'}_i}, \boldsymbol{e^{'}_j})
= a^{'}_i a^{'}_j f(P_{ip}\boldsymbol{e_p}, P_{jq}\boldsymbol{e_q})
= a^{'}_i a^{'}_j P_{ip}P_{jq} f(\boldsymbol{e_p}, \boldsymbol{e_q})
= a^{'}_i a^{'}_j P_{ip}P_{jq} l_{pq}.
\end{align*}

Comparing with \eqref{bilinear_transform} we arrive to:

\begin{align}
l^{'}_{ij} &= P_{ip}P_{jq} l_{pq} . \label{bilinear_transform1}
\end{align}

For every $\boldsymbol{a} \in L$ and $\boldsymbol{b} \in L$ the form $f(\boldsymbol{a}, \boldsymbol{b})$ is symetric if $f(\boldsymbol{a}, \boldsymbol{b}) = f(\boldsymbol{b}, \boldsymbol{a})$, and antisymmetric if $f(\boldsymbol{a}, \boldsymbol{b}) = -f(\boldsymbol{b}, \boldsymbol{a})$.
Even more, if the form is symetric than $l_{ij} = l_{ji}$, and for antisymetric  $l_{ij} = -l_{ji}$, i.e. the matrix of the bilinear form is symertic or atisymetric.

Again, the value of the bilinear form do not change if the basis is changed.

\section{Quadratic froms}

The \emph{quadratic form} is a particular case of the bilinear form where the coefficients matrix is symetric and the arguments are the same, i.e. $f(\boldsymbol{a}, \boldsymbol{a})$ and $l_{ij} = j_{ji}$. The bilinear function thus becomes a function of a single argument $f(\boldsymbol{a})$. Taking in consideration the symetric matrx we can write the from as:

\begin{align*}
f(\boldsymbol{a}) = l_{11}a_{1}^2 + 2l_{12}a_1a_2 + ... + 2l_{1n}a_1a_n + \\
+ l_{22}a_{2}^2 + 2l_{23}a_2a_3 + ... + 2l_{2n}a_2a_n + \\
+ ................. + \\
+ l_{nn}a_{n}^2
\end{align*}

If there is basis in $L$ for which all the coefficients $l_{ij}$ of the quadratic form becomes zero for $i \neq j$ than this quadratic form has a \emph{canonical representation}:

\begin{align}
    f(\boldsymbol{a}) &= l_{ii}a_{1}^2 + l_{22}a_{2}^2 + ... + l_{nn}a_{n}^2 = l_{ii}a_{i}^2.
\end{align}

Using the linear transformation the quadratic from can take a normal form of the following:

\begin{align}
    f(\boldsymbol{a}) &= a_{1}^2 + a_{2}^2 + ... + a_{k}^2 - a_{k+1}^2 - ... - a_{r}^2, \textnormal{ where } r <= n.  
\end{align}

\begin{theorem}
The number of the positive and negative terms in the quadratic form do not change if the basis is changed.
\end{theorem}

This theorem is called  \emph{Sylvester's law of inertia} of quadratic forms.

\chapter{Dual Space}

\section{Dual space and reciprocal basis}

Let's for a linear space $L$ the all possible linear forms $f(\boldsymbol{a})$ are defined over the $L$. These linear functions will from a set $L^*$ which also will be a linear space, i.e. a linear space the elements (vectors) of which will be functions. The linear space $L^*$ is called \emph{dual space} and have the same dimension as of $L$.

In the linear space $L$ let's take a basis $E(\boldsymbol{e_1}, \boldsymbol{e_2}, .., \boldsymbol{e_n})$. Because the set $L^*$ is also a linear space we can choose in it a basis too $E^{*}(\boldsymbol{e^{1}}, \boldsymbol{e^{2}}, .., \boldsymbol{e^{n}})$\footnote{We use the upper script to note the basis vectors of the dual space.}. The basis $E$ and $E^{*}$ will have the same number of basis vectors because their linear space are of the same dimension. The difference is that the elements of the basis $E^*$ are functions of elements from $L$, i.e. $E^{*}(\boldsymbol{e^{1}}(\boldsymbol{a}), \boldsymbol{e^{2}}(\boldsymbol{a}), .., \boldsymbol{e^{n}}(\boldsymbol{a}))$.

Now we will choose a base $E^*$ in a such way that:

\begin{align}
\boldsymbol{e}^{i}(\boldsymbol{e}_j) = \delta^{i}_j. \label{dual_basis}
\end{align}

The basis $E^{*} \in L^{*}$ which satisfy the relation \eqref{dual_basis} is called \emph{reciprocal basis} or \emph{dual basis} to the basis $E \in L$. This is not the same as reciprocal vectors in the space of geometric vectors where both sets are in the same space. The vectors of $E^{*}$ are in a different space which is a function space isomprphic to $L$. But later we will see in what case these two concepts will mean the same thing.

\section{Transformation of dual basis}

Let's there be a basis $E \in L$ and its reciprocal basis $E^{*} \in L^*$. If we move in $L$ to another basis $E^{'}$ the dual basis also will change to a new one $E^{*}^{'}$. The question is what would be the relation between the basis vectors of $E^{*}$ and $E^{*}^{'}$ if we give the matrix $P_{ij}$ for transformation \eqref{eqn:basis_transfromation1} between the $E^{'}$ and $E$.

Because the dual space $L^*$ is a linear space and because $E^{*}$ is a basis in it the transformation of the basis vectors $E^{*}$ must be in accordance with \eqref{eqn:basis_transfromation1} but with a different matrix $D_{ij}$. Because of the condition \eqref{dual_basis} there must be a relationship between $D_{ij}$ and $P_{ij}$ which we will find out. Thus, for now we can write:

\begin{align*}
\boldsymbol{e^{i}^{'}} = D_{ij}\boldsymbol{e}^{j}.
\end{align*}

On the other hand talking in consideration the condition \eqref{dual_basis} we can write

\begin{align}
\delta^{i}_j = \boldsymbol{e}^{i}^{'}(\boldsymbol{e}^{'}_j) = P_{ik}\boldsymbol{e}^{k}(\boldsymbol{e}^{'}_j)
= D_{ik}\boldsymbol{e}^{k}(P_{jm}\boldsymbol{e}_m)
= D_{ik}P_{jm}\boldsymbol{e}^{k}(\boldsymbol{e}_m) = D_{ik}P_{jk}.
\end{align}

Finally:

\begin{align}
 D_{ik}P_{jk} = \delta^{i}_j.
\end{align}

The above relation can be written in the matrix from as

\begin{align}
 DP^{T} = I.
\end{align}

And the matrix D is

\begin{align}
 D = (P^T)^{-1} = Q.
\end{align}

Thus, the transformation of dual basis will be:

\begin{align}
\boldsymbol{e^i} = Q_{ij}\boldsymbol{e^{j}}. \label{dual_basis_transfromation}
\end{align}

Thus, if we are given the transformation matrix $P_{ij}$ for basis vectors from $L$ the transformation of the dual basis vectors will transform the same way as the coordinates of a vectors from $L$, i.e accordingly with \eqref{eqn:transfromation_coordinates1}.

Now if we are given basis $E \in L$ and the matrix $P_{ij}$, we want to see how the coordinates of a vector $\boldsymbol{a} \in L^{*}$ expressed in terms of dual basis $E^*$ changes when the $E^*$ is changed\footnote{Actually $E^*$ is changing due to the change of $E$.}. Let's write the expansion of the vector $\boldsymbol{a}$\ in dual basis $E^*$ and $E^{*}^{'}$:

\begin{align*}
\boldsymbol{a} &= a_{i}\boldsymbol{e}^{i} = a_{i}^{'}\boldsymbol{e}^{i}^{'}. \label{dual_basis_expantion} \\
\end{align*}

Using \eqref{dual_basis_transfromation} we can write:

\begin{align*}
a_{i}^{'}\boldsymbol{e}^{i}^{'} &= a_{i}^{'}Q_{ij}\boldsymbol{e}^{j} = Q_{ji} a_{i}^{'}\boldsymbol{e}^{i}.
\end{align*}

Taking in consideration \eqref{dual_basis_expantion} we have:

\begin{align*}
a_{i} = Q_{ji}a_{j}^{'},
\end{align*}

or in matrix from:

\begin{align*}
A &= Q^{T}A^{'}, \\
A^{'} &= (Q^{T})^{-1}A = PA.
\end{align*}

Thus, finally

\begin{align}
a_{i}^{'} = P_{ij}a_{j} \label{dual_baisis_coodinates_transfrom}
\end{align}

We can see that the \eqref{dual_baisis_coodinates_transfrom} is the same as transformation of the basis vectors of $E \in L$, i.e. the same as \eqref{eqn:basis_transfromation1}.

\section{Covariant and contravariant}
\label{section:covariant_contravariant}

In this section we will summarize the low of transformation for basis vectors $E$ and $E^*$, and for the coordinates of the vectors from $L$ and $L^*$. Also we will make some changes in the notations.
Starting from here the coordinates of a vector in $L$ we will note with superscript $a^1, a^2,...,$, the basis vectors in $L$ we will note with lower script as $\boldsymbol{e_1}, \boldsymbol{e_2},...$.
When we refer to the dual space $L^*$ than the basis vectors will remain as it was with superscript $\boldsymbol{e^1}, \boldsymbol{e^2},...$, but the components of the vector in $L^*$ will note with lower script as $a_1, a_2,...,$. 

Thus, for the basis $E \in L$ and coordinates of a vector $a \in L$ the low of transformation is

\begin{align}
\boldsymbol{e}_{i} &= P_{ij}\boldsymbol{e}_{j}, \label{covariant_transformation_low} \\
a^{i} &= Q_{ij}a^{j}. \label{contravariant_transformation_low}
\end{align}

The low of transformation for the corresponding dual basis $E^{*} \in L^{*}$ and coordinates of a vector $a \in L^*$ will be:

\begin{align}
\boldsymbol{e}^{i} &= Q_{ij}\boldsymbol{e}^{j}, \label{dual_contravariant_transformation_low} \\
a_{i} &= P_{ij}a_{j}. \label{dual_covariant_transformation_low}
\end{align}

The values which are transformed as \eqref{covariant_transformation_low} are said that they are \emph{covariant} values. Those values which transforms in accordance with the low \eqref{contravariant_transformation_low} are said the they are \emph{contravariant} values. Thus, the  basis vectors from \eqref{covariant_transformation_low} are called covariant basis vectors, and the basis vectors from \eqref{dual_contravariant_transformation_low} are called contravariant basis vectors. The vector components from \eqref{dual_covariant_transformation_low} are called covariant components and the vector components form \eqref{contravariant_transformation_low} are called  contravariant components of the vector.

\chapter{Tensors and Groups}

In this chanter we will introduce two concepts tensors and groups which will help us to understand better the subject when we will make another step into building a space. Also, these notions are often used in Physics.

\section{Tensors}

A \emph{tensors} is a set of real numbers which are defined over the space $L$, and which transforms in a certain way when the the basis is changed. These number are called the \emph{coordinates of the tensor}. Usually this set of numbers depending of the \emph{order} of the tensor are presented as a number, vector matrix, or square matrix.

Tensors of 0-order are scalars defined over the space. The coordinates of a scalar is a single number which do not change if the basis is changed.

The 1st-order tensors are a set of numbers defined over the space which can be represented as vector matrix, and which transforms as contravariant or covariant values\footnote{See the section \ref{section:covariant_contravariant} about covariant and contravariant values.}. The coordinates of the 1st-order tensors are noted with superscrip $(\epsilon^1, \epsilon^2,...,\epsilon^n)$ if they transform as contravariant values, and with subscript $(\epsilon_1, \epsilon_2,...,\epsilon_n)$ if they transform as covariant values.

The simplest example of 1st-order tensor are vectors from $L$ or $L^*$. The coordinates of the vectors $(a^1, a^2, ..., a^n)$ transforms under the low \eqref{contravariant_transformation_low}, i.e. as contravariant values. The coordinates of the vector $(a_1, a_2, ..., a_n)$ from the corresponding dual space $L^*$ transform under the low \eqref{dual_covariant_transformation_low}, i.e. as covariant values.

The second order tensor a more interesting because their coordinates ca be represented as a matrix, for example as $T^{ij}$ or $T_{ij}$, and the low of transformation of the coordinates of the 2nd-order tensor are the following:

\begin{align}
{T^{'ij}} &= Q_{ik}Q_{jl}T^{kl} \\ 
T^{'}_{ij} &= P_{ik}P_{jl}T_{kl}. \label{2nd_cov_tensor_transform}
\end{align}

But there might also:

\begin{align}
{T^{i}_{j}}^{'} &= Q_{ik}P_{jl}T^{k}_{l} 
\end{align}

In general the tensors with order > 2 are sets of number which transform as:

\begin{align}
T^{'ij...k} &= Q_{ip}Q_{jq}...Q_{ks}T^{pq...s}, \\
T^{'}_{ij...k} &= P_{ip}P_{jq}...P_{ks}T_{pq...s}, \\
T^{'ij...k}_{mn...p} &= Q_{iq}Q_{js}...Q_{kr}P_{mv}P_{nu}...P_{pw}T^{qs...r}_{vu...w}.
\end{align}

\section{Groups}

A \emph{group} is a set $G$ of elements of any nature for which the notion if identity is given. Also, the operation upon the elements is defined, and this operation is called \emph{multiplication}.

Let's have two elements $a \in G$ and $b \in G$. The multiplication operation maps these two elements $a$ and $b$ to a new element $c \in G$, and this operation is written as:

\[c = ab\].

It is said the element $c$ is the product of $a$ and $b$.

\begin{definition}
A group is a set $G$ for which the multiplication of elements is defined and for which the following axioms holds:

\begin{axiom}
For any elements $a, b, c \in G$ 

\[(ab)c = a(bc)\].
\end{axiom}
\begin{axiom}
There exist an element $e \in G$ called the unitary element of the group such that for any element $a \in G$

\[ae = a.\]
\end{axiom}
\begin{axiom}
For any element $a \in G$ there exist an element $x \in G$ such that

\[ax = e.\]

The element $x$ is called the inverse of $a$ and it is noted as $a^{-1}$.
\end{axiom}
\end{definition}

A group will be called \emph{abelian group} if for any two elements $a, b \in G$ the following relation holds:

\begin{align*}
ab = ba.
\end{align*}

TODO: about group of transfromation.

\chapter{Euclidean space}

Until this chapter we've made two major steps into building our space. We defined the liner space $L$ which contained only vectors. Than a next major step was by defining the affine space in which a new set $\mathbb{A}$ (the set of points) was added, and the map between $L$ and $\mathbb{A}$. Anyway, in the linear and affine space we could not have the notion of distance and angles in space, i.e. these spaces didn't have a metric. The next step would be to add more to the geometry of the affine space with axioms which will transform it into an euclidean space. The euclidean space can be said that it resembles the properties of our real space.

\section{Inner product}

An euclidean space is an affine space for which along with the axioms I, II, and III it follows also the these additional sets of axioms of (IV):

\begin{axiom}
Over the linear space $L$ a bilinear form $g(\boldsymbol{a}, \boldsymbol{b})$ if defined. This function $g$ is called the inner product of $\boldsymbol{a}$ and $\boldsymbol{b}$, and it is noted as (\boldsymbol{a}, \boldsymbol{b}). Thus, the function $g$ for every two vectors $\boldsymbol{a}, \boldsymbol{b} \in L$ maps in a certain way a real number, i.e $(\boldsymbol{a}, \boldsymbol{b}) = g(\boldsymbol{a}, \boldsymbol{b})$.
\end{axiom}

\begin{axiom}
The inner product is commutative:

\[(\boldsymbol{a}, \boldsymbol{b}) = (\boldsymbol{b}, \boldsymbol{a})\].

This means that the bilinear from $g$ is symmetric.
\end{axiom}

\begin{axiom}
The inner product is distributive:

\[(\boldsymbol{a}, \boldsymbol{b} + \boldsymbol{c}) = (\boldsymbol{a}, \boldsymbol{b}) + g(\boldsymbol{a}, \boldsymbol{c}) \].

This actually follows from the definition of bilinear form.
\end{axiom}

\begin{axiom}
The inner product is homogene:

\[(\alpha\boldsymbol{a}, \boldsymbol{b}) = \alpha g(\boldsymbol{a}, \boldsymbol{b})\].

This, also, follows from he definition of bilinear form.
\end{axiom}

\begin{axiom}
The inner product is nonsingular. If $(\boldsymbol{a}, \boldsymbol{b}) = 0$ for any $b \in L$, than $\boldsymbol{a} = \boldsymbol{0}$.

This means that the bilinear from $g$ is nonsingular.
\end{axiom}

\begin{axiom}
For every vector $\boldsymbol{a} \in L$ the inner product

\[(\boldsymbol{a}, \boldsymbol{a}) \geq 0. \]
\end{axiom}


As we can see for the definition of euclidean space we do not add anything new in terms of elements, they remain the same -- the set of vectors $L$ and the set of points $\mathbb{A}$. The new axioms IV are just additional rules over these two sets\footnote{Actually, new rules over the set $L$.} which define the concept of inner product.

According to the definition of bilinear form we can express the inner product as follows:

\begin{align}
(\boldsymbol{a}, \boldsymbol{b}) = g_{ij}a^jb^j \label{ineer_product_bilinear_from}. 
\end{align}

\begin{definition}
Two vectors $\boldsymbol{a}, \boldsymbol{b} \in L$ are said the they are orthogonal if their inner product is zero, i.e.

\[(\boldsymbol{a}, \boldsymbol{b}) = 0.\]
\end{definition}

\section{Norm of a vector}

\begin{definition}
The norm of a vector $\boldsymbol{a} \in L$ will be the number

\[\norm{\boldsymbol{a}} = \sqrt{(\boldsymbol{a}, \boldsymbol{a})}.\]

\end{definition}

For the euclidean space the norm of a vector will be a real number\footnote{There other cases when the inner product (\boldsymbol{a}, \boldsymbol{a}) might be not positive, thus the norm is be imaginary. Also, there are cases when the norm of a vector $\norm{\boldsymbol{a}} = 0$ even if the vector $\boldsymbol{a} \neq \boldsymbol{0}$. These kind of vectors are called \emph{izotopic} vectors. About this case we will discuss later in this book}.

The norm of a vector also refers to the length of a vector. Thus, we arrived to the concept of distance in space. Because in affine space every two points $A, B \in \mathbb{A}$ a vector is associated, than the distance between the points $A$ and $B$ will be associated with the norm (length) of the vector.

Because the inner product is bilinear form $g$, thus, for the cases when we have the inner product (\boldsymbol{a}, \boldsymbol{a}) the bilinear form $g$ will become a quadratic from.

The quadratic form $\norm{\boldsymbol{a}}^2 = (\boldsymbol{a}, \boldsymbol{a}) = g(\boldsymbol{a}, \boldsymbol{a})$ is called the \emph{metric form} of the space. In accordance with the definition of the quadratic from the metric form can be written as

\begin{align}
\norm{\boldsymbol{a}}^2 = g_{ij}a^ia^j. \label{matric_quadratic_from}  
\end{align}

\section{Ortonormal basis}

\section{The metric tensor}

According to \eqref{bilinear_transform1} the coefficients $g_{ij}$ of the bilinear from \eqref{ineer_product_bilinear_from} and the quadratic from \eqref{matric_quadratic_from} transforms the same way as the coordinates of a second order tensor \eqref{2nd_cov_tensor_transform}. Thus, the coefficients $g_{ij}$ will define a second order covariant tensor, called the \emph{metric tensor} of the space. This tensor is noted as $g$.

Let's have a inner product of two vectors $(\boldsymbol{a}, \boldsymbol{b})$. Also, let's have a basis $E$. The inner product than can be expressed as:

\begin{align}
(\boldsymbol{a}, \boldsymbol{b}) = (a^i \boldsymbol{e_i}, b^j\boldsymbol{e_j}) = a^{i}b^{j} (\boldsymbol{e_i}, \boldsymbol{e_j}) = g_{ij} a^{i}b^{j}. \label{inner_and_metrics}
\end{align}

We can see the if we have a basis E, we also can define the metric of the tensor as:

\[g_{ij} = (\boldsymbol{e_i}, \boldsymbol{e_j})\].

Since the bilinear form (\boldsymbol{a}, \boldsymbol{b}) which define the inner product is symmetric also the metric tensor $g$ is symmetric.

Also, we can see from \eqref{inner_and_metrics} that if we are given the metric tensor $g$ of the space, than the inner product will be defined too. From the inner product also we can define the norm, i.e. the distance in space. Thus, the metric tensor of the space is enough to define the inner product and the distance in space, i.e. the metrics of the space. According to this in order to have an euclidean space we define a metric tensor $g$ in the affine space\footnote{In general the metric tensor $g$ will make an affine space into a \emph{metric space}. But if $g$ is defined in a such way that the quadratic form of the norm also is positive, than we are defining an euclidean space.}.

\section{Distance in Space}

We would like to explore how the introduction of the metric tensor $g$ in the affine space transforms it into an Euclidean space and thus defines the concept of distance in space. The structure of the Euclidean space consists of a set of vectors $L$ and a set of points $\mathbb{A}$, which map to vectors. This mapping is inherited from the affine space. Hence, in the Euclidean space, we have points, and it is natural to ask what the distance between these points is.

Let's consider two points $X$ and $Y$, and an origin point $O$\footnote{This origin is the same origin from the affine space, i.e., the Euclidean space inherits this concept.}. The coordinates of points $X$ and $Y$ are ${x^1, x^2, ..., x^n}$ and ${y^1, y^2, ..., y^n}$, respectively. These two points also define a vector $\overline{XY}$. Upon introducing the metric tensor $g$, we can define the norm of this vector as follows:

\begin{align}
\norm{\overline{XY}} = \sqrt{(\overline{XY}, \overline{XY})}.
\end{align}

Since $(\overline{XY}, \overline{XY})$ is also the metric quadratic form, in accordance with \eqref{matric_quadratic_from} and \eqref{vector_span_affine_coordiantes}, we can write:

\begin{align}
(\overline{XY}, \overline{XY}) &= g_{ij}(y^i - x^i) (y^j - x^j),
\end{align}

and the norm becomes:

\begin{align}
\norm{\overline{XY}} &= \sqrt{g_{ij}(y^{i} - x^{i})(y^{j} - x^{j})}. \label{distance_in_space}
\end{align}

The coefficients in this equation are the coordinates of the metric tensor $g$. Since the norm of the vector is also defined as its length, equation \eqref{distance_in_space} shows us the relationship between the coordinates of the points $X$ and $Y$ and the length of the vector. In accordance with the mapping between the vector set and the point set stated in the axioms of the affine space, equation \eqref{distance_in_space} also defines the relationship between points $X$ and $Y,$ which can be defined as the distance between them.

For example, in a real space, the set $\mathbb{A}$ will correspond to points in space, and equation \eqref{distance_in_space} along with the relationship between points imposed by the mapping to vectors will define how these points must be arranged in space.

Let's now see how the distance in space changes if we change the system of coordinates, i.e., move from $(O, \boldsymbol{e_1}, \boldsymbol{e_2}, ..., \boldsymbol{e_n})$ to $(O, \boldsymbol{e_1}^{'}, \boldsymbol{e_2}^{'}, ..., \boldsymbol{e_n}^{'})$\footnote

\begin{align}
\norm{\overline{XY}} &= \sqrt{\delta^{k}{m} \delta^{l}{n} P_{ik} Q_{im} P_{jl} Q_{in} g_{kl}(y^{m} - x^{m})(y^{n} - x^{n})} \
&= \sqrt{P_{ik} Q_{im} \delta^{m}{n} P{jl} Q_{in} g_{kl}(y^{m} - x^{m})(y^{n} - x^{n})} \
&= \sqrt{P_{ik} Q_{im} P_{jl} Q_{in} g_{mn}(y^{m} - x^{m})(y^{n} - x^{n})}.
\end{align}

In the above expression, $\delta^{m}_{n}$ is the Kronecker delta symbol, which takes the value 1 if $m=n$ and 0 otherwise.

Now, we can recognize the right-hand side of Equation (10) as the metric quadratic form written in the new coordinate system $(O, \boldsymbol{e_1}^{'}, \boldsymbol{e_2}^{'}, ..., \boldsymbol{e_n}^{'})$. This can be seen by comparing Equation (10) to Equation (3). Hence, we have

\begin{align}
\norm{\overline{XY}} &= \sqrt{g^{'}_{mn} (y^{m} - x^{m})(y^{n} - x^{n})}. \label{distance_in_new_coordinate_system}
\end{align}

Therefore, the distance between the two points $X$ and $Y$ in the new coordinate system $(O, \boldsymbol{e_1}^{'}, \boldsymbol{e_2}^{'}, ..., \boldsymbol{e_n}^{'})$ is given by Equation (11).

The above discussion shows that the distance between two points in space remains unchanged under a coordinate transformation, i.e., it is invariant. This is a desirable property, as it should not depend on the choice of the coordinate system. The metric tensor provides a means to measure the distance between two points in space in a way that is invariant under coordinate transformations.

This concludes our discussion on how the introduction of the metric tensor $g$ in the affine space defines the concept of distance in space.

\section{Metrical isophormism}

\section{Examples of euclidean space}

\section{The volume of object in euclidean spaces}

\section{Tensors in euclidean space}

\section{The group of euclidean rotation}

\chapter{Pseudo-euclidean space}

In the last chapter we've put the past bricks into building a space that can be said that has the same geometric properties as our real world space. Because of this it can by used in Physics as a mathematical tool to describe our real space, and write physical laws with it. But if we would limit us only to to euclidean space than from the point view of Physics it can describe only the classical interpretation of our space which is an approximation real space we are living. To describe even more accurate we need for a mathematical space which is euclidean, but has spacial defined metric, i.e. the metric tenor is defined as follows:

\[g_{ij} = (\boldsymbol{e_i}, \boldsymbol{e_j})\], where (\boldsymbol{e_0}, \boldsymbol{e_0}) = -1, and, (\boldsymbol{e_j}, \boldsymbol{e_i}) = 0 for {i != j} \]

An euclidean space which a such metric will become a so-called pseudo-euclidean space of n dimensions\footnote{More exactly it a pseudo-euclidean space of index 1. This is because due to the metric tensor metric quadratic form is of the from }. Thus, the Special Relativity theory says that our space differs from the euclidean space and to describe it more accurately we need to use the pseudo-euclidean space from mathematics. Our classical interpretation of the space usually is described mathematically by an euclidean  space with 3 dimension. The Special Relativity which describe even more accurate our space needs to use the the pseudo-euclidean space with dimension 4. Later we will see that one of the dimension of pseudo-euclidean used in Special Relativity is labeled as time, but this do not change things and for now we will explore mathematically some interesting and strange properties of a such space which reflates the same behaviour our real space describe by Special Theory Relativity.

\section{Pseudo-euclidean space of two dimensions}

- Diagram, asymtots, etc.

- Rashevsky, p 176, sec 44

- Rozendorn , p 310 (maybe also write about hyperbolic transformation, see p 316)

- Angle between vectors

- Triangle inequality

- Isotopic vectors}

- Transformation in pseudo-euclidean space

\section{Minkowski spacetime}

\cite{rosenfeld_noneucldean}, p.525


\begin{thebibliography}{}
\bibitem{efimov}
Efimov, N.V., Rozendorn, E. R. \emph{Linear Algebra and Multi-Dimensional Geometry}, Moscow: Nauka, 1970.

\bibitem{rashevsky}
Rashevsky, P.K., \emph{Riemannian Geometry and Tensor Analysis.}, 3rd ed., Moscow: Nauka, 1967.
   
\bibitem{korush}
Kurosh, A.G., \emph{Higher Algerba}, 10th ed., Moscow, Nauka, 1971. 

\bibitem{rosenfeld_noneucldean}
   Rosenfeld, B.A.,  \emph{Non-euclidean Spaces}, 3rd ed., Moscow: Lenand, 2020.

\bibitem{rosenfeld_ndim}
Rosenfeld, B.A.,  \emph{Multi-dimensional Spaces}, 3rd ed., Moscow: Lenand, 2020.

\bibitem{hobson}
Riley, K.F., Hobson, M.P., Bence, S.J., \emph{Mathematical Methods for Physics and Engineering}, 3rd ed., Cambridge: Cambridge University Press, 2006.

\bibitem{landau_field}
Landau, L.D. , Lifshitz, E.M. \emph{Field Theory}, Vol. 2, 9th ed., Moscow, Fizmatlit, 2020.

\bibitem{dirac}
Dirac, P.A.M, \emph{The Principles of Quantum Mechanics}, 3rd ed., Oxford, Claredon Press, 1948.


\end{thebibliography}

\end{document}

