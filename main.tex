\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[amsbook]{}
\usepackage[T2A]{fontenc}
\usepackage[russian]{}
\usepackage[hebibliography]{}
\usepackage{amssymb}
\usepackage{draftwatermark}
\SetWatermarkText{Draft}
\SetWatermarkScale{5}
\SetWatermarkColor[gray]{0.95}
\usepackage{physics}{}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{axiom}{ยบ}

\title{Building a Space}
\author{Iurie Nistor}
\date{September 2021}

\begin{document}

\maketitle

\newpage
\emph{This work is under CC0 1.0 Universal https://creativecommons.org/publicdomain/zero/1.0/)}

\emph{To the extent possible under law, Iurie Nistor has waived all copyright and related or neighboring rights to Building a Space.}

\tableofcontents

\section{Foreword}

\section{Notations}

$L_n$ --  linear space of dimension $n$
\newline
$E_n$ --  affine space of dimension $n$
\newline
$R_n$ --  euclidean space of dimension $n$
\newline
$\{a, b, c, d...\}$ -- a set with elements $a, b, c, d, ...$

\chapter{Linear spaces}

In order to build a space we will start with the most abstract thing we can imagine - a \emph{set}, and let's call it $L$. The elements of the set $L$ can be of any nature: numbers, vectors, functions, anything, and named with letters $\{\textbf{a}, \textbf{b}, \textbf{c}, \textbf{d}...\}$. It is better we for now to try refrain from imagining them be something concrete, for example, as vectors we know from classic geometry books. Let pretend we don't know what they are, what to do with them, and how they behave.

\begin{section}{Axioms of linear space}

We will start now to define how the elements of the set $L$ behave, these rules are called \emph{the axioms of linear space}. Also these axioms are grouped as follows:


I: Axioms of sum

II: Axioms of multiplication to a number

Thus, we will define how to \emph{sum} the elements and how to \emph{multiply} them to real numbers.

Another concept related to the elements of $L$ is \emph{identity}. Thus, the elements of $L$ are \emph{identical} if they belong to the same class, i.e they have the same properties. Thus, we say that two elements $\textbf{a}$ and $\textbf{b}$ are identical (or equal) if they belong to $L$, have the same properties, and we note this $\textbf{a} = \textbf{b}$.

\begin{subsection}{I: Axioms of sum}

\begin{axiom} For every element $\textbf{a}$ and $\textbf{b}$ from $L$ the sum $\textbf{c} = \textbf{a} + \textbf{b}$ is defined in a such way that $\textbf{c}$ also belongs to $L$.
\end{axiom}

\begin{axiom}The sum of elements is commutative, i.e. for every $\textbf{a}$ and $\textbf{b}$ from $L$
$\textbf{a} + \textbf{b} = \textbf{b} + \textbf{a}$.
\end{axiom}

\begin{axiom} The sum of elements are associative, i.e. for every $\textbf{a}$, $\textbf{b}$ and $\textbf{c}$ from $L$
$(\textbf{a} + \textbf{b}) + \textbf{c} = \textbf{a} + (\textbf{b} + \textbf{c})$.
\end{axiom}

\begin{axiom} There exists an element {$\textbf{0}$} in $L$ such that for every $a$ from $L$  $\textbf{a} + \textbf{0} = \textbf{a}$.
\end{axiom}

\begin{axiom} For every element $\textbf{a}$ from $L$ there exists an element {$-\textbf{a}$} in $L$ such that $\textbf{a} + (-\textbf{a}) = \textbf{0}$.
\end{axiom}

\end{subsection}
\begin{subsection}{II: Axioms of multiplication to a number}
\begin{axiom} For every element $\textbf{a}$ from $L$ and for every real number $\alpha$ an element $\textbf{b}$ from $L$ is given as $\textbf{b} = \alpha \textbf{a}$, which is called the multiplication of $\textbf{a}$ to the number $\alpha$.
\end{axiom}

\begin{axiom} The multiplication of every element $\textbf{a}$ from $L$ to the number 1 is the same element \textbf{a}, i.e. $\textbf{a} \cdot 1 = \textbf{a}$.
\end{axiom}

\begin{axiom} The multiplication of elements of $L$ to a number is distributive relative to the sum of numbers, i.e. $(\alpha + \beta)\textbf{a} = \alpha \textbf{a} + \beta \textbf{a}$.
\end{axiom}

\begin{axiom}
The multiplication of elements of $L$ to a number is distributive relative to the sum of elements, i.e. $\alpha (\textbf{a} + \textbf{b}) = {\alpha} \textbf{a} + {\alpha} \textbf{b}$.
\end{axiom}

\begin{axiom} The multiplication of elements of $L$ to a number is associative, i.e. $\alpha (\beta \textbf{a}) = (\alpha \beta) \textbf{a}$.
\end{axiom}
\end{subsection}

Introduction of these axioms, i.e. rules upon the elements adds the first glimpses of our space we want to build, i.e. these first glimpses will transform the abstract set $L$ into a \emph{linear space}. Thus, here is the definition:

\emph{A set $L$ the elements of which follows the axioms of group I and II is called linear space $L$.}

A linear space, also, is also called \emph{vector space}, and the elements of $L$ are called \emph{vectors}. Due to this, starting from now, we will call the elements of the linear space $L$ vectors. Let's mention that even if the we them vectors they not necessarily represent vectors as we know from classic geometry.

If the axioms of multiplication define the multiplication of vectors with real numbers, than $L$ is called \emph{real linear space}. If the multiplication of vectors are with complex numbers than $L$ will be \emph{complex linear space}.
\end{section}

\section{Examples of linear spaces}

\subsection{Vectors on a two dimensional infinite plane}

Imagine all vectors (those defined like in the classic geometry books) which can be drawn on a infinite plane. Let's see if the set of these vectors $V$ forms a linear space. In order to verify this, there is a need two show that all vectors of the plane $P$ follow the axioms of the linear space.

Let's verify the fist axiom from the group I and II. If we draw on the plane $P$ two vectors $\textbf{a}$ and $\textbf{b}$ and sum them by using the rule of parallelogram we find another vector $\textbf{c} = \textbf{a} + \textbf{b}$ and which lies in the same plane, i.e. $c \in V$. If we draw a vector $\textbf{d}$ and multiply them to a number $\alpha$ we will find another vector $\textbf{e}$ which is collinear to $\textbf{d}$ and lies on the same plane, i.e. $\textbf{d} \in V$.

The rest of the axioms can be verified the same way by using the rules for vectors from the classic geometry. Thus, all vectors from an infinite plane form a linear space.

Because, for example, values from Physics like velocities, accelerations, forces and others are denoted and behave same way as vectors they form linear spaces too.

\subsection{The space of real numbers}

The set of all real numbers $R$ form a linear space. Indeed, for any two numbers from this set we can find the sum which is in the same set. Also, there is \textbf{0} vector which is 0, and for every number we can find its opposite number by adding the sing '-' before the number. Multiplication to a number also hods. Thus, we can see that the elements of the set $R$ forms a linear space.

\subsection{The space of matrices}

\subsection{Null space}

\subsection{The space of functions of a single variable}

Let's be there a set $F$ the elements of which are all functions of a single variable $x$. The set $F$ forms a vector space.

To verify the axiom of sums (I) we take two elements $f_a(x) \in F$ and $f_b(x) \in F$ and define a new function $f_c(x) = f_a(x) + f_b(x)$. It can seen that $f_c(x)$ is also a function of a single variable $x$ and $f_c(x) \in F$. For the rest of the axioms of sum are obvious.

The multiplication axioms (II) are also easy to verify. For any $f(x) \in F$ and a real number $\alpha$ we can define a new function $f_\alpha (x) = \alpha \cdot f(x)$ which $f_\alpha (x) \in F$. For the rest of the axioms of multiplication are obvious too.

Note that the elements of $F(f_a(x), f_b(x), f_c(x)...)$ are called vectors too, they form a vector space but is not really intuitive to image them as vectors.

Functions which form vector spaces are used as a mathematical tool in Quantum Mechanics. These kind of spaces are called Hilbert spaces\footnote{Hilbert space is actually an infinite dimensional linear space\cite{hobson}. We will see later what is a dimension of a linear space.}, the vectors of which often are noted with $\ket{a}, \ket{b}, \ket{c}...$, called ket vectors\cite{dirac} and which represent the state of a quantum system. Thus, the state of a quantum system forms a linear space and, because the axioms I are hold, than it means the the state of a quantum system can be expressed as a sum of other states\footnote{In Quantum Mechanics it is called the principle of superposition of quantum states.\cite{dirac}}.

\subsection{Vectors on a finite plane}

All vectors of a finite plane $P$ do not form a vector space. Indeed, we can take two vectors $\textbf{a}$ and $\textbf{b}$ from the plane in a such way that sum $\textbf{c} = \textbf{a} + \textbf{b}$ will not fit $\textbf{c}$ into to plane, i.e $\textbf{c} \notin P$. Thus, at least the first axiom of the group I doesn't hold.

\subsection{Natural numbers}

\section {Some corollaries to the axioms of a linear space}

TEXT HERE

\section{Linear combination and dependence}

Because the axioms of linear space defines how we can sum elements and multiply to a real number now we can define some more complex concepts related to linear space. 

Let's have a finite number of elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ and any real numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$. If any element $\boldsymbol{b} \in L$ is defined as

\[\boldsymbol{b} = \alpha_1\boldsymbol{a_1} + \alpha_2\boldsymbol{a_2} + \alpha_3\boldsymbol{a_3} + ... + \alpha_k\boldsymbol{a_n},\]

we say that $\boldsymbol{b}$ is expressed as a \emph{linear combination} of $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_k}$. If all the numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$ are zero than the linear combination is called \emph{trivial linear combination}, otherwise (if at least one number is not zero) it is called \emph{non-trivial linear combination}.

Any elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ are \emph{linearly dependent} if there exists a non-trivial combination such that

\begin{equation}
\label{eqn:dependence}
\alpha_1\boldsymbol{a_1} + \alpha_2\boldsymbol{a_2} + \alpha_3\boldsymbol{a_3} + ... + \alpha_n\boldsymbol{a_n} = \boldsymbol{\theta}.
\end{equation}

On the other side the elements $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ are \emph{linear independent} if \eqref{eqn:dependence} holds only for the trivial combination, i.e. only when the real numbers $\alpha_1, \alpha_2, \alpha_3,..., \alpha_n$ are all equal to zero.

From these two above definitions it follows that if any set $\boldsymbol{a_1}, \boldsymbol{a_2}, \boldsymbol{a_3},..., \boldsymbol{a_n}$ from $L$ which are linear dependent there exists an element $\boldsymbol{a_k}$ from this set which can be expressed as a liner combination of the rest:

\[\boldsymbol{a_k} = \frac{\alpha_1}{\alpha_k}\boldsymbol{a_1} + \frac{\alpha_2}{\alpha_k}\boldsymbol{a_2} + \frac{\alpha_3}{\alpha_k}\boldsymbol{a_3} + ... +  \frac{\alpha_{k-1}}{\alpha_k}\boldsymbol{a_{k-1}} + \frac{\alpha_{k+1}}{\alpha_k}\boldsymbol{a_{k+1}} + ... + \frac{\alpha_n}{\alpha_k}\boldsymbol{a_n} ,\]

where $\alpha_k \neq 0$. Indeed, because the elements are linear depended than their combination is non-trivial, i.e. there must exist at least one element $a_k$ for which $\alpha_k$ is not null. 

For any set of elements from $L$ which are linear independent, than no element from this set can be expressed as a linear combination of others.

\begin{section}{The dimension and basis of the linear space}

The linear space $L$ is said to be $n-dimensional$ if there exist a finite $n$ elements from $L$ which are linear independent and every other number of elements which is bigger than $n$ is linear dependent.

The linear space would be an \emph{infinite dimensional linear space} if for any integer $N > 0$ we can find $N$ linear independent elements from $L$.

As we can see the dimension of a linear space is not the same as how many elements are in $L$.

If the elements $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ from $L$ are linear independent and every other element $\boldsymbol{a}$ from $L$ can be expressed as a linear combination: 
\begin{equation}
\label{eqn:basis}
\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + a_3\boldsymbol{e_3} + ... + a_n\boldsymbol{e_n},
\end{equation}
than $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ is called to from a \emph{basis} in $L$.

Every element which is expressed in the form of \ref{eqn:basis} is said that is expressed as a linear combination of the basis $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$. The coefficients $a_1, a_2, a_3,..., a_n$ are called the coordinates of the element $\boldsymbol{a}$ in this basis.

The basis $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ is not unique, there can be a lot of other set of elements which will form a basis.

We should not think $\boldsymbol{e_1}, \boldsymbol{e_2}, \boldsymbol{e_3}, ..., \boldsymbol{e_n}$ as unitary geometric vectors. They might be if we consider the elements to be geometric vectors and the basis with unitary vectors like $\boldsymbol{i}, \boldsymbol{j}, \boldsymbol{k}$. For example, in the three dimensional space of geometric vectors any three vectors (not necessarily orthogonal and unitary) which are not all coplanar can form a basis, i.e. every other vectors can be expressed in this basis.

Thus, in a $n$-dimensional linear space $L$ we can take any $n$ linear independent elements (if such exists) and form a basis. Also, these theorem are true:

\emph{The linear space has the dimension $n$ if and only if there exists a basis of n elements.}

\emph{If there exists a basis in $L$ than any element $\boldsymbol{a}$ which is not part of the basis is expressed uniquely in this basis, i.e. the coordinates of the element $\boldsymbol{a}$ in this basis are unique.}

Also here we have another important theorem:

\emph{The coordinates of sum of two vectors are the sum of coordinates of these vectors. If a vector is multiplied by a real number, also the coordinates of the vector are multiplied by this number.}

To be more clear if we have

\[\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + a_3\boldsymbol{e_3} + ... + a_n\boldsymbol{e_n},\]
\[\boldsymbol{b} = b_1\boldsymbol{e_1} + b_2\boldsymbol{e_2} + b_3\boldsymbol{e_3} + ... + b_n\boldsymbol{e_n}\]

than

\[\boldsymbol{a} + \boldsymbol{b} = (a_1 + b_1)\boldsymbol{e_1} + (a_2 + b_2)\boldsymbol{e_2} + ... + (a_n + b_n)\boldsymbol{e_n},\]
\[{\alpha}\boldsymbol{a} = ({\alpha}a_1)\boldsymbol{e_1} + ({\alpha}a_2)\boldsymbol{e_2} + ... + ({\alpha}a_n)\boldsymbol{e_n}.\]

Also, it follows that all the coordinates of the null element are zero, i.e:

\[\boldsymbol{\theta} = 0{\cdot}\boldsymbol{e_1} + 0{\cdot}\boldsymbol{e_2} + ... + 0{\cdot}\boldsymbol{e_n}.\]
\end{section}

\section{Isomorphism}

Let's have linear spaces $L$ and $L^'$. The elements of $L$ and $L^'$ might be of the same or different nature \footnote{For example, $L(\boldsymbol{v_1}, \boldsymbol{v_2}, \boldsymbol{v_3},...)$ might be a space of vectors and $L^{'}(f_1(x), f_2(x), f_3(x) ...)$ a space of functions of a single variable.}. Also, there a bidirectional unique mapping between the elements $L$ and ${L^{'}}$, i.e. for every element $\boldsymbol{a}$ from $L$ we assign an element $\boldsymbol{a^{'}}$ from $L$ and the opposite. The mapping is done in a such way that the the sum of any two elements $\boldsymbol{a}$ and $\boldsymbol{b}$ from $L$ maps to the sum of $\boldsymbol{a^{'}}$ and $\boldsymbol{b^{'}}$ from $L^{'}$, and the multiplication of any element $\boldsymbol{a}$ from $L$ to a real number $\alpha$ maps to the multiplication of $\boldsymbol{a^{'}}$ from $L^{'}$ to the same number, i.e.

\begin{equation}
\label{eqn:isomorphism1}
(\boldsymbol{a} + \boldsymbol{b})^{'} = \boldsymbol{a^{'}} + \boldsymbol{b^{'}},
\end{equation}

\begin{equation}
\label{eqn:isomorphism2}
(\alpha \cdot \boldsymbol{a})^{'} = \alpha \cdot \boldsymbol{a^{'}}.
\end{equation}

The spaces $L$ and $L^{'}$ are said to be \emph{isomorphic} if they are mapped and the mapping is done in a such way that \eqref{eqn:isomorphism1} and \eqref{eqn:isomorphism2} holds\footnote{The linear spaces must be both real or complex. If one space is real and another complex, the spaces are not isomorphic because \eqref{eqn:isomorphism2} doesn't hold.}.

\begin{theorem}
For every n all real n-dimensional linear spaces are isomorphic between each other. For every n all complex n-dimensional linear spaces are isomorphic between each other.
\end{theorem}

\begin{theorem}
The linear space which is isomorphic to a n-dimensional linear space is n-dimensional too.
\end{theorem}

Here are also two corollaries which follows from these theorems:

\begin{corollary}
Finite dimensional linear space with different dimension are not isomorphic.
\end{corollary}

For example, the space of geometric vectors on an infinite plane is of dimension two. The linear space of vectors which are on an infinite line is of dimension one. These two linear spaces are not isomorphic.

\section{Linear subspaces}

Let's have a linear space $L$. Also, let's take a set of elements from $L$ and call it $\widetilde{L}$.

\begin{definition}
The set $\widetilde{L}$ of elements from $L$ is called linear subspace of $L$ if the axioms of addition and multiplication hold upon the set $\widetilde{L}$.
\end{definition}

This would mean that for every element $\boldsymbol{a}$ and $\boldsymbol{b}$ from $\widetilde{L}$ the sum $\boldsymbol{c} = \boldsymbol{a} + \boldsymbol{b}$ belongs to $\widetilde{L}$ too. The result of multiplication of an element $\boldsymbol{a}$ from $\widetilde{L}$ to a real number also belongs to $\widetilde{L}$, i.e if $\boldsymbol{b} = \alpha\cdot\boldsymbol{a}$ than $\boldsymbol{b} \in \widetilde{L}$.

Following the definition:

\begin{theorem}
Every linear subspace $\widetilde{L}$ of a linear space $L$ also is a linear space.
\end{theorem}

This is true because every $\widetilde{L}$ satisfies the axioms I and II.

To understand better how $\widetilde{L}$ looks like let's take the example of the geometric vectors on an infinite plane. At first glance the word "subspace" makes us to think that if we will take a finite region, for example, a circle on this plane than that would be a subspace. It is not true because all vectors that can be put on this region will not satisfy the axioms of linear space. What would be than a linear subspace? An infinite line that lies on the plane would be a linear space. Indeed, all vectors which can lie on this line will satisfy the axioms of linear space, and all these vectors also lie on the plane, i.e. they are a subset of linear space of vectors on the plane.

Another trivial example of linear subspace will be the null element $\boldsymbol{\theta}$ of a linear space.

\section{Linear transformation}

\subsection{Einstein notation}

Before starting the subject of linear transformation let's define some notations which are called Einstein summation convention\footnote{This kind of notation is often used in Physics. It was introduced by Albert Einstein in 1916.}. The reason of introduction of such notation is because we can write mathematical formulas in a more efficient way\footnote{For example, the Physics equations will look more simple and clear.}. Let's write some examples by defing the notation.

\begin{equation}
\label{eqn:enstein_notation1}
a_kb_k := \sum_{k=1}^{n} a_kb_k = a_1b_1 + a_2b_2 + ... + a_nb_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation2}
a_{ik}b_k := \sum_{k=1}^{n} a_{ik}b_k = a_{i1}b_1 + a_{i2}b_2 + .. + a_{in}b_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation3}
a_{ij}b_j := \sum_{k=1}^{n} a_{ij}b_j = a_{i1}b_1 + a_{i2}b_2 + .. + a_{in}b_n,
\end{equation}

\begin{equation}
\label{eqn:enstein_notation4}
a_{ij}b_{jk}c_k := \sum_{j=1}^{m}\sum_{k=1}^{n} a_{ij}b_{jk}c_k.
\end{equation}

Einstein notation says that if the same index is
found in different terms, than the summation
is done over that index which is called the \emph{dummy}
index. Thus, instead of writing sums like $\sum_{k=1}^{n} a_kb_k$ we use a short notation $a_kb_k$ instead. Thus, in \eqref{eqn:enstein_notation1} $k$ is the dummy index over which the summation is done and because is found in both terms $a$ and $b$. In \eqref{eqn:enstein_notation2} the dummy index is $j$ over which the summation is done and 
because is found in both terms. In \eqref{eqn:enstein_notation3} we have two dummy indexes $j$ and $k$.

The name of the dummy index can be changed the result will be unchanged. For example

\[a_{ij}b_j = a_{iq}b_q,\]

because we in both cases we will have to sum starting from $1$ to $n$. The only think this kind of notation doesn't tell us is from which number to start and end the sum. Thus, using this kind of notation there should be an agreement upon this.

\subsection{The basis and coordinates transformation}

Let bet there a linear space $L_n$. The lower index means that it $n$-dimensional. The basis in this space will be $e_1, e_2, ..., e_n$. Because there may be a lot of other sets of elements from $L_n$ that can from a basis we are interested if we change the basis how the coordinates of an element from $L_n$ are changed.

First of all let express the new basis vectors in terms of old basis:

\begin{equation}
\label{eqn:basis_transfromation}
\begin{split} 
\boldsymbol{e^{'}_1} &= P_{11}\boldsymbol{e_1} + P_{12}\boldsymbol{e_2} + ... + P_{1n}\boldsymbol{e_n} \\
\boldsymbol{e^{'}_2} &= P_{21}\boldsymbol{e_1} + P_{22}\boldsymbol{e_2} + ... + P_{2n}\boldsymbol{e_n} \\
 &\;\;\vdots \\
\boldsymbol{e^{'}_n} &= P_{n1}\boldsymbol{e_1} + P_{n2}\boldsymbol{e_2} + ... + P_{nn}\boldsymbol{e_n}
\end{split}
\end{equation}

Using the Einstein notation we now can write this transformation as:

\begin{equation}
\label{eqn:basis_transfromation1}
\boldsymbol{e^{'}_i} = P_{ij}\boldsymbol{e_j}.
\end{equation}

The equation \eqref{eqn:basis_transfromation1} actually shows how basis vectors are changed if the basis is changed. The the coefficients $P_{ij}$ form a matrix which is called the transformation matrix of basis vectors. On the other hand knowing a basis and if we take any matrix $P$ in a such way that 

\[Det P \neq 0\],

than using \eqref{eqn:basis_transfromation1} we can find a new basis in $L_n$.

Transformation \eqref{eqn:basis_transfromation1} can be written in matrix notation of we choose to note the set of basis vectors a on column matrix:

\begin{equation}
\label{eqn:basis_transfromation2}
\boldsymbol{E^{'}} = P\boldsymbol{E}.
\end{equation}

Now let's see how the coordinates of a vector is are changed if we changed the basis. Let there be vector $\boldsymbol{a} \in L_n$. Because the vectors $\boldsymbol{a}$ itself doesn't change if the basis is changed than we can express this vectors in old and new basis as follows:

\begin{align*}
\boldsymbol{a} &= a_i\boldsymbol{e_i} \\ \boldsymbol{a} &= a^{'}_i\boldsymbol{e{'}_i}.
\end{align*}

In the second expression using \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e{'}_i}$ we can write:

\[\boldsymbol{a} &= a^{'}_i(P_{ij}\boldsymbol{e_j}) = a^{'}_i(P_{ji}\boldsymbol{e_i}) = (a^{'}_{i}P_{ji})\boldsymbol{e_i} \]\footnote{Here we sum over dummy index $i$ and $j$ because they are met in different terms, i.e. $\sum_{i=1}^{n} \sum_{j=1}^{n}a^{'}_iP_{ij}\boldsymbol{e_j}$ }.

Comparing this with the first expression we find the expression if old coordinates of a vectors in the of new coordinates:

\begin{equation}
\label{eqn:transfromation_coordinates}
a_i = P_{ji}a^{'}_j.
\end{equation}

From \eqref{eqn:transfromation_coordinates} we can see the the transformation matrix for coordinates is the transposed $P^T$ matrix $P$ of \eqref{eqn:basis_transfromation1}. If we not the collection of coordinates as one column matrix, we can write \eqref{eqn:transfromation_coordinates} by using matrix notation:

\begin{equation}
\label{eqn:transfromation_matrix1}
A = P^{T}A^{'}.
\end{equation}

If we want to express the new coordinates in terms of old we can write:

\begin{equation}
\label{eqn:transfromation_matrix2}
A^{'} = (P^{T})^{-1}A.
\end{equation}

If $(P^T)^{-1}$ is noted as $Q$ than

\begin{equation}
\label{eqn:transfromation_matrix3}
A^{'} = QA.
\end{equation}

Or using coordinates

\begin{equation}
\label{eqn:transfromation_coordinates1}
a{'}_i = Q_{ij}a_j.
\end{equation}

Comparing \eqref{eqn:basis_transfromation2} we can see the the coordinates of a vector from $L_n$ transforms using a different transformation matrix $(P^T)^{-1}$ when we move from an old basis $\boldsymbol{E}$ to a new one $\boldsymbol{E^'}$. This matrix can be obtained from the $P$ by taking the inverse of the transpose. Also, $det ((P^T)^{-1}) \neq 0$.

If we are given the matrix $(P^T)^{-1}$ we can find $P$ and then find the new basis by using \eqref{eqn:basis_transfromation2}. i.e. if we have the transformation law for vector coordinates we can find the transformation law for the basis.

\chapter{Affine space}

\section{The definition of affine space}
Now another step is taken and add more to the construction of the space,
i.e. to its geometry. For example, if only a subset of vectors from the linear space
is taken than with it we cannot define objects. There is a need for something more.
Beside the linear space $L$ there is given another set $\mathbb{A}$ the elements of which are called \emph{points} and can be of any nature. The points of this set be noted as A, B, C.. etc.

The \emph{affine space} is the set $\mathbb{A}$ of points for which the mapping of points to vectors from the linear space $L$ are stated by the following axioms (grouped as III):

\begin{axiom}
For every two points A and B from $\mathbb{A}$ we map a vector \boldsymbol{a} from L as follows:

\[\overline{AB} = \boldsymbol{a}.\]

If order of the points are taken as \overline{BA}, than

\[\overline{BA} = -\boldsymbol{a}.\]
\end{axiom}

\begin{axiom}
For every point $A \in \mathbb{A}$ and every vector $\boldsymbol{a} \in L$ an unique point $B \in \mathbb{A}$ is found such that

\[\overline{AB}\ = \boldsymbol{a}.\]

\end{axiom}

\begin{axiom}
If for any points A, B, C from $\mathbb{A}$ is mapped $\overline{AB} = \boldsymbol{a}$, $\overline{BC} = \boldsymbol{b}$ than $\overline{AC}$ will map to $\boldsymbol{a} + \boldsymbol{b}$, i.e.  $\overline{AC} = \boldsymbol{a} + \boldsymbol{b}$.
\end{axiom}

The dimension of the affine space is the dimension of its linear space $Ln$. The affine space is real or complex if the linear space $L$ is real or complex.

\section{Affine coordinates}

Let's have an affine space $\mathbb{A}$ of dimension $n$. Let's take point $O \in \mathbb{A}$ which will be called \emph{origin of affine coordinates}. Also, in the corresponding linear space $L$ a basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$ is taken. If another point $P \in \mathbb{A}$ is taken than $\overline{OP}$ will correspond to vector from $L_n$.
If we expand this vector $\overline{OP}$ in the basis than:

\begin{equation}
\overline{OP} = x_1\boldsymbol{e_1} + x_2\boldsymbol{e_2} + ... + x_n\boldsymbol{e_n} = x_i\boldsymbol{e_i}.
\end{equation}

The coefficients $x_1, x_2, ..., x_n$ are called the \emph{affine coordinates} of the point $P$ relative to the affine origin $O$. Thus, the affine system of coordinates always is defined by the chosen point $O$ and the basis $\{\boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n}\}$. Let's note this system as $(O, \boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n})$.

\section{Transformation of affine coordinates}

Let's have two points A and B with coordinates ${a_1, a_2, ..., a_n}$ and ${b_1, b_2, ..., b_n}$ than we are interested what will be the coordinates of the vector $\overline{AB}$ expressed as coordinates of the points $A$ and $B$. We can write the following:

\[\overline{AB} = \overline{OB} - \overline{OA} = b_i\boldsymbol{e_i} - a_i\boldsymbol{e_i} = (b_i - a_i)\boldsymbol{e_i}. \] 

Let not see how the coordinates of a point $A$ changes if we move the origin $O$ to another point $O_1$. The coordinates of the point $O_1$ relative old origin will be ${o_1, o_2, ..., o_n}$. The coordinates of the point $A$ relative the origin $O$ will be ${a_1, a_2, ..., a_n}$. Let's find the coordinates ${a{^'}_1, a{^'}_2, ..., a{^'}_n}$ of the point $A$ relative to the new origin $O_1$. First we write the vector relation:

\[\overline{OA} = \overline{OO_1} + \overline{O_1A}\]

Now let's write this in term of coordinates:

\begin{align}
a_i\boldsymbol{e_i} = o_i\boldsymbol{e_i} + a^{'}_i\boldsymbol{e_i} = (o_i + a^{'}_i)\boldsymbol{e_i}.\label{affine_transfrom}
\end{align}

Than we obtain:

\begin{align}
a_i &= o_i + a^{'}_i \label{affine_origin_move} \\
a^{'}_i &= a_i - o_i. \label{affine_origin_move1}
\end{align}

Suppose that we move from one coordinates system $(O, \boldsymbol{e_1}, \boldsymbol{e_1}, ..., \boldsymbol{e_n})$ to another $(O_1, \boldsymbol{e^{'}_1}, \boldsymbol{e^{'}_1}, ..., \boldsymbol{e^{'}_n})$, i.e. we change not only the origin but also the basis.

Again lets write the vector relation which doesn't depend of the affine coordnates system:

\[\overline{OA} = \overline{OO_1} + \overline{O_1A}\]

Now we expand the vectors $\overline{OA}$ and $\overline{OO_1}$ in the old system but $\overline{O_1A}$ in the new system. The relation will hold too because the vectors are not changed. Thus we obtain something similar to \eqref{affine_transfrom}:

\begin{align}
a_i\boldsymbol{e_i} = o_i\boldsymbol{e_i} + a^{'}_i\boldsymbol{e^{'}_i} \label{affine_transfrom1}.
\end{align}


Using the \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e^{'}_i}$ in \eqref{affine_transfrom1} we can write:

\begin{align}
a_i\boldsymbol{e_i} &= a^{'}_{i}P{ij}\boldsymbol{e_j} + o_i\boldsymbol{e_i} = P{ji}a^{'}_{i}\boldsymbol{e_i} + o_i\boldsymbol{e_i} \label{affine_transfrom2}.\\
a_i &= Pjia^{'}_i + o_j. \label{affine_transfromation1}
\end{align}

Now let's write \eqref{affine_transfromation1} in the matrix from:

\begin{align}
A &= P^{T}A^{'} + O \\
\end{align}

From the matrix equation we can find matrix $A^'$:

\begin{align*}
A^{'} = (P^T)^{-1}A - (P^T)^{-1}O = QA - QO.
\end{align*}

In index notation this will be:

\begin{align}
a^{'}_i = Q_{ij}a_j - Q_{ij}o_j \label{general_affine_tr}
\end{align}

If the basis is not changed and only the origin $O$, it means that the matrix Q is unitary, thus,

\[a^{'}_i = I_{ij}a_j - I_{ij}o_j = a_i - o_i.\]

Thus, we arrived to \eqref{affine_origin_move1}.

If the basis is changed and the origin is the same, than in \eqref{general_affine_tr} all coefficients $o_j$ are zero than

\[a^{'}_i = Q_{ij}a_j\],

thus, we arrived to \eqref{eqn:transfromation_coordinates1}.

\section{Objects in affine space}

From our experience we know that in our real space we have objects, in classic geometry they are called geometric object. It is natural for as to ask since we are talking about a space what are the objects in this space? How do they look like? Because in the affine space we added a set of points beside the linear vector space $L$, it is possible know to define what would be a object in affine space. In general, an object D in the affine space would consist of a set of points $D$ from $\mathbb{A}$.

\section{The volume of object in affine space}

\chapter{Linear and bilinear forms}

\section{Linear form}

Let's have a linear space $L$. For every vectors $\boldsymbol{a} \in L$ we associate a real number. In this way we are defining a scalar function of vector argument $f(\boldsymbol{a})$ over the space $L$. This function will be called $linear$ if it satisfies the following relations:

\begin{align}
f(\boldsymbol{a} + \boldsymbol{b}) &= f(\boldsymbol{a}) + f(\boldsymbol{b}) \textnormal{ for every } \boldsymbol{a} \in L \textnormal{ and } \boldsymbol{b} \in L\textnormal{,} \label{linearfunction_1} \\
f(\alpha\boldsymbol{a}) &= \alpha f(\boldsymbol{a}) \textnormal{ for every vector } \boldsymbol{a} \in L \textnormal{ and any real number } \alpha. \label{linearfunction_2}
\end{align}

If the space $L$ is complex than the function value $f(\boldsymbol{a})$ will be a complex number.

Now let's have in $L$ a basis $\mathscr{E}$ and a vector $\boldsymbol{a} \in L$ expressed in this basis:

\begin{align*}
\boldsymbol{a} = a_1\boldsymbol{e_1} + a_2\boldsymbol{e_2} + ... + a_n\boldsymbol{e_n}.
\end{align*}

The linear function $f(\boldsymbol{a})$ taking in consideration \eqref{linearfunction_1} and \eqref{linearfunction_2} can be expressed as:

\begin{align*}
f(\boldsymbol{a}) = a_{1}f(\boldsymbol{e_2}) + a_{2}f(\boldsymbol{e_2}) + ... + a_{n}f(\boldsymbol{e_n}) = a_if(\boldsymbol{e_i}). \label{linear_from0}
\end{align*}

If we write the coefficients $f(\boldsymbol{e_i})$ as $l_i$ than

\begin{align}
f(\boldsymbol{a}) = l_{1}a_{1} + l_{2}a_{2} + ... + l_{n}a_{n} = l_{i}a_{i}. \label{linear_from1}
\end{align}

The expression \eqref{linear_from1} is a polynomial of degree 1 relative to $a_i$ and it is called \emph{linear from}.

Now let's see how the coefficients $l_i$ transforms if we are moving to another base $E^'$. In the base $E^'$ we have:

\begin{align}
f(\boldsymbol{a}) &= f(a^{'}\boldsymbol{e^{'}_i}) = a^{'}_if(\boldsymbol{e^{'}_i}) = a^{'}_{i}l^{'}_i \label{linear_from_new_basis}
\end{align}

In accordance with \eqref{eqn:basis_transfromation1} instead of $\boldsymbol{e^{'}_i}$ we write:

\begin{align*}
f(\boldsymbol{a}) &= a^{'}_if(\boldsymbol{e^{'}_i}) = a^{'}_if(P_{ij}\boldsymbol{e_j}) =  a^{'}_iP_{ij}f(\boldsymbol{e_j}) = a^{'}_iP_{ij}l_j.\label{linear_from0}
\end{align*}

Comparing with \eqref{linear_from_new_basis} we can see that the expression of transformation for coefficients $l_i$ is:

\begin{align}
l_i^{'}&= P_{ij}l_j,\label{covariant_transformation}
\end{align}

i.e. they transform like basis vectors.

It is worth mentioning that the value of function $f(\boldsymbol{a})$ doesn't change if the basis is changed.

\section{Bilinear form}

If for every two vectors $\boldsymbol{a} \in L$ and $\boldsymbol{b} \in L$ we associate a real number, than we are defining a function of two vector arguments $f(\boldsymbol{b}, \boldsymbol{a})$ over the space $L$. The function $f(\boldsymbol{b}, \boldsymbol{a})$ will be called \emph{bilinear} if is is linear in respect to every its argument, i.e:

\begin{align*}
f(\boldsymbol{a_1} + \boldsymbol{a_2}, \boldsymbol{b}) &= f(\boldsymbol{a_1}, \boldsymbol{b}) + f(\boldsymbol{a_2}, \boldsymbol{b}), \\
f(\alpha\boldsymbol{a_1}, \boldsymbol{b}) &= \alpha f(\boldsymbol{a}, \boldsymbol{b}), \\
f(\boldsymbol{a}, \boldsymbol{b_1} + \boldsymbol{b_2}) &= f(\boldsymbol{a}, \boldsymbol{b_1}) + f(\boldsymbol{a}, \boldsymbol{b_2}), \\
f(\boldsymbol{a}, \alpha\boldsymbol{b}) &= \alpha f(\boldsymbol{a}, \boldsymbol{b}).
\end{align*}

If we use the above relation than we can expand $f(\boldsymbol{a}, \boldsymbol{b})$ as follows:

\begin{align*}
f(\boldsymbol{a}, \boldsymbol{b}) = f (a_i\boldsymbol{e_i}, b_j\boldsymbol{e_j}) = a_{i}b_{j}f(\boldsymbol{e_i}, \boldsymbol{e_j}) = l_{ij}a_{i}b_{j} .
\end{align*}

The expression $l_{ij}a_{i}b_{j}$ is called \emph{bilinear form}. The coefficients $l_{ij}$ are called \emph{coefieints of the bilinear form} and they can be written as a matrix, called the \emph{matrix of the bilinear form}.

Now let's see how the coefficients $l_{ij}$ transform with change of basis. In the new basis

\begin{align}
f(\boldsymbol{a}, \boldsymbol{b})
= f (a^{'}_i\boldsymbol{e^{'}_i}, b^{'}_j\boldsymbol{e^{'}_j})
= a^{'}_i b^{'}_j f(\boldsymbol{e^{'}_i}, \boldsymbol{e^{'}_j})
= l^{'}_{ij} a^{'}_i b^{'}_j  . \label{bilinear_transform}
\end{align}

Using \eqref{eqn:basis_transfromation1} we can write:

\begin{align*}
f(\boldsymbol{a}, \boldsymbol{b}) &= a^{'}_i a^{'}_j f(\boldsymbol{e{'}_i}, \boldsymbol{e^{'}_j})
= a^{'}_i a^{'}_j f(P_{ip}\boldsymbol{e_p}, P_{jq}\boldsymbol{e_q})
= a^{'}_i a^{'}_j P_{ip}P_{jq} f(\boldsymbol{e_p}, \boldsymbol{e_q})
= a^{'}_i a^{'}_j P_{ip}P_{jq} l_{pq}.
\end{align*}

Comparing with \eqref{bilinear_transform} we arrive to:

\begin{align}
l^{'}_{ij} &= P_{ip}P_{jq} l_{pq} . \label{bilinear_transform1}
\end{align}

For every $\boldsymbol{a} \in L$ and $\boldsymbol{b} \in L$ the form $f(\boldsymbol{a}, \boldsymbol{b})$ is symetric if $f(\boldsymbol{a}, \boldsymbol{b}) = f(\boldsymbol{b}, \boldsymbol{a})$, and antisymmetric if $f(\boldsymbol{a}, \boldsymbol{b}) = -f(\boldsymbol{b}, \boldsymbol{a})$.
Even more, if the form is symetric than $l_{ij} = l_{ji}$, and for antisymetric  $l_{ij} = -l_{ji}$, i.e. the matrix of the bilinear form is symertic or atisymetric.

Again, the value of the bilinear form do not change if the basis is changed.

\section{Quadratic froms}

The \emph{quadratic form} is a particular case of the bilinear form where the coefficients matrix is symetric and the arguments are the same, i.e. $f(\boldsymbol{a}, \boldsymbol{a})$ and $l_{ij} = j_{ji}$. The bilinear function thus becomes a function of a single argument $f(\boldsymbol{a})$. Taking in consideration the symetric matrx we can write the from as:

\begin{align*}
f(\boldsymbol{a}) = l_{11}a_{1}^2 + 2l_{12}a_1a_2 + ... + 2l_{1n}a_1a_n + \\
+ l_{22}a_{2}^2 + 2l_{23}a_2a_3 + ... + 2l_{2n}a_2a_n + \\
+ ................. + \\
+ l_{nn}a_{n}^2
\end{align*}

If there is basis in $L$ for which all the coefficients $l_{ij}$ of the quadratic form becomes zero for $i \neq j$ than this quadratic form has a \emph{canonical representation}:

\begin{align}
    f(\boldsymbol{a}) &= l_{ii}a_{1}^2 + l_{22}a_{2}^2 + ... + l_{nn}a_{n}^2 = l_{ii}a_{i}^2.
\end{align}

Using the linear transformation the quadratic from can take a normal form of the following:

\begin{align}
    f(\boldsymbol{a}) &= a_{1}^2 + a_{2}^2 + ... + a_{k}^2 - a_{k+1}^2 - ... - a_{r}^2, \textnormal{ where } r <= n.  
\end{align}

\begin{theorem}
The number of the positive and negative terms in the quadratic form do not change if the basis is changed.
\end{theorem}

This theorem is called  \emph{Sylvester's law of inertia} of quadratic forms.

\chapter{Dual Space}

\section{Dual space and reciprocal basis}

Let's for a linear space $L$ the all possible linear forms $f(\boldsymbol{a})$ are defined over the $L$. These linear functions will from a set $L^*$ which also will be a linear space, i.e. a linear space the elements (vectors) of which will be functions. The linear space $L^*$ is called \emph{dual space} and have the same dimension as of $L$.

In the linear space $L$ let's take a basis $E(\boldsymbol{e_1}, \boldsymbol{e_2}, .., \boldsymbol{e_n})$. Because the set $L^*$ is also a linear space we can choose in it a basis too $E^{*}(\boldsymbol{e^{1}}, \boldsymbol{e^{2}}, .., \boldsymbol{e^{n}})$\footnote{We use the upper script to note the basis vectors of the dual space.}. The basis $E$ and $E^{*}$ will have the same number of basis vectors because their linear space are of the same dimension. The difference is that the elements of the basis $E^*$ are functions of elements from $L$, i.e. $E^{*}(\boldsymbol{e^{1}}(\boldsymbol{a}), \boldsymbol{e^{2}}(\boldsymbol{a}), .., \boldsymbol{e^{n}}(\boldsymbol{a}))$.

Now we will choose a base $E^*$ in a such way that:

\begin{align}
\boldsymbol{e}^{i}(\boldsymbol{e}_j) = \delta^{i}_j. \label{dual_basis}
\end{align}

The basis $E^{*} \in L^{*}$ which satisfy the relation \eqref{dual_basis} is called \emph{reciprocal basis} or \emph{dual basis} to the basis $E \in L$. This is not the same as reciprocal vectors in the space of geometric vectors where both sets are in the same space. The vectors of $E^{*}$ are in a different space which is a function space isomprphic to $L$. But later we will see in what case these two concepts will mean the same thing.

\section{Transformation of dual basis}

Let's there be a basis $E \in L$ and its reciprocal basis $E^{*} \in L^*$. If we move in $L$ to another basis $E^{'}$ the dual basis also will change to a new one $E^{*}^{'}$. The question is what would be the relation between the basis vectors of $E^{*}$ and $E^{*}^{'}$ if we give the matrix $P_{ij}$ for transformation \eqref{eqn:basis_transfromation1} between the $E^{'}$ and $E$.

Because the dual space $L^*$ is a linear space and because $E^{*}$ is a basis in it the transformation of the basis vectors $E^{*}$ must be in accordance with \eqref{eqn:basis_transfromation1} but with a different matrix $D_{ij}$. Because of the condition \eqref{dual_basis} there must be a relationship between $D_{ij}$ and $P_{ij}$ which we will find out. Thus, for now we can write:

\begin{align*}
\boldsymbol{e^{i}^{'}} = D_{ij}\boldsymbol{e}^{j}.
\end{align*}

On the other hand talking in consideration the condition \eqref{dual_basis} we can write

\begin{align}
\delta^{i}_j = \boldsymbol{e}^{i}^{'}(\boldsymbol{e}^{'}_j) = P_{ik}\boldsymbol{e}^{k}(\boldsymbol{e}^{'}_j)
= D_{ik}\boldsymbol{e}^{k}(P_{jm}\boldsymbol{e}_m)
= D_{ik}P_{jm}\boldsymbol{e}^{k}(\boldsymbol{e}_m) = D_{ik}P_{jk}.
\end{align}

Finally:

\begin{align}
 D_{ik}P_{jk} = \delta^{i}_j.
\end{align}

The above relation can be written in the matrix from as

\begin{align}
 DP^{T} = I.
\end{align}

And the matrix D is

\begin{align}
 D = (P^T)^{-1} = Q.
\end{align}

Thus, the transformation of dual basis will be:

\begin{align}
\boldsymbol{e^i} = Q_{ij}\boldsymbol{e^{j}}. \label{dual_basis_transfromation}
\end{align}

Thus, if we are given the transformation matrix $P_{ij}$ for basis vectors from $L$ the transformation of the dual basis vectors will transform the same way as the coordinates of a vectors from $L$, i.e accordingly with \eqref{eqn:transfromation_coordinates1}.

Now if we are given basis $E \in L$ and the matrix $P_{ij}$, we want to see how the coordinates of a vector $\boldsymbol{a} \in L^{*}$ expressed in terms of dual basis $E^*$ changes when the $E^*$ is changed\footnote{Actually $E^*$ is changing due to the change of $E$.}. Let's write the expansion of the vector $\boldsymbol{a}$\ in dual basis $E^*$ and $E^{*}^{'}$:

\begin{align*}
\boldsymbol{a} &= a_{i}\boldsymbol{e}^{i} = a_{i}^{'}\boldsymbol{e}^{i}^{'}. \label{dual_basis_expantion} \\
\end{align*}

Using \eqref{dual_basis_transfromation} we can write:

\begin{align*}
a_{i}^{'}\boldsymbol{e}^{i}^{'} &= a_{i}^{'}Q_{ij}\boldsymbol{e}^{j} = Q_{ji} a_{i}^{'}\boldsymbol{e}^{i}.
\end{align*}

Taking in consideration \eqref{dual_basis_expantion} we have:

\begin{align*}
a_{i} = Q_{ji}a_{j}^{'},
\end{align*}

or in matrix from:

\begin{align*}
A &= Q^{T}A^{'}, \\
A^{'} &= (Q^{T})^{-1}A = PA.
\end{align*}

Thus, finally

\begin{align}
a_{i}^{'} = P_{ij}a_{j} \label{dual_baisis_coodinates_transfrom}
\end{align}

We can see that the \eqref{dual_baisis_coodinates_transfrom} is the same as transformation of the basis vectors of $E \in L$, i.e. the same as \eqref{eqn:basis_transfromation1}.

\section{Covariant and contravariant}

In this section we will summarize the low of transformation for basis vectors $E$ and $E^*$, and for the coordinates of the vectors from $L$ and $L^*$. Also we will make some changes in the notations.
Starting from here the coordinates of a vector in $L$ we will note with superscript $a^1, a^2,...,$, the basis vectors in $L$ we will note with lower script as $\boldsymbol{e_1}, \boldsymbol{e_2},...$.
When we refer to the dual space $L^*$ than the basis vectors will remain as it was with superscript $\boldsymbol{e^1}, \boldsymbol{e^2},...$, but the components of the vector in $L^*$ will note with lower script as $a_1, a_2,...,$. 

Thus, for the basis $E \in L$ and coordinates of a vector $a \in L$ the low of transformation is

\begin{align}
\boldsymbol{e}_{i} &= P_{ij}\boldsymbol{e}_{j}, \label{covariant_transformation_low} \\
a^{i} &= Q_{ij}a^{j}. \label{contravariant_transformation_low}
\end{align}

The low of transformation for the corresponding dual basis $E^{*} \in L^{*}$ and coordinates of a vector $a \in L^*$ will be:

\begin{align}
\boldsymbol{e}^{i} &= Q_{ij}\boldsymbol{e}^{j}, \label{dual_contravariant_transformation_low} \\
a_{i} &= P_{ij}a_{j}. \label{dual_covariant_transformation_low}
\end{align}

The values which are transformed as \eqref{covariant_transformation_low} are said that they are \emph{covariant} values. Those values which transforms in accordance with the low \eqref{contravariant_transformation_low} are said the they are \emph{contravariant} values. Thus, the  basis vectors from \eqref{covariant_transformation_low} are called covariant basis vectors, and the basis vectors from \eqref{dual_contravariant_transformation_low} are called contravariant basis vectors. The vector components from \eqref{dual_covariant_transformation_low} are called covariant components and the vector components form \eqref{contravariant_transformation_low} are called  contravariant components of the vector.

\chapter{Tensors}

\chapter{Groups}

\chapter{Euclidean space}

\section{Axioms of euclidean space}

\section{Inner product}

\section{Norm of a vector}

\section{Ortonormal basis}

\section{The metric tensor}

\section{Metrical isophormism}

\section{Examples of euclidean space}

\section{The volume of object in euclidean spaces}

\section{Tensors in euclidean space}

\section{The group of euclidean rotation}

\chapter{Minkowski space}

\section{Spheres}

\cite{rosenfeld_noneucldean}, p.524

\section{Angles between vectors}

\cite{rosenfeld_noneucldean}, p.525

\section{Spacetime}

\begin{thebibliography}{}
\bibitem{efimov}
Efimov, N.V., Rozendorn, E. R. \emph{Linear Algebra and Multi-Dimensional Geometry}, Moscow: Nauka, 1970.

\bibitem{rashevsky}
Rashevsky, P.K., \emph{Riemannian Geometry and Tensor Analysis.}, 3rd ed., Moscow: Nauka, 1967.
   
\bibitem{korush}
Kurosh, A.G., \emph{Higher Algerba}, 10th ed., Moscow, Nauka, 1971. 

\bibitem{rosenfeld_noneucldean}
   Rosenfeld, B.A.,  \emph{Non-euclidean Spaces}, 3rd ed., Moscow: Lenand, 2020.

\bibitem{rosenfeld_ndim}
Rosenfeld, B.A.,  \emph{Multi-dimensional Spaces}, 3rd ed., Moscow: Lenand, 2020.

\bibitem{hobson}
Riley, K.F., Hobson, M.P., Bence, S.J., \emph{Mathematical Methods for Physics and Engineering}, 3rd ed., Cambridge: Cambridge University Press, 2006.

\bibitem{landau_field}
Landau, L.D. , Lifshitz, E.M. \emph{Field Theory}, Vol. 2, 9th ed., Moscow, Fizmatlit, 2020.

\bibitem{dirac}
Dirac, P.A.M, \emph{The Principles of Quantum Mechanics}, 3rd ed., Oxford, Claredon Press, 1948.


\end{thebibliography}

\end{document}

